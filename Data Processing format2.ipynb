{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import layoutparser as lp\n",
    "import cv2\n",
    "\n",
    "# Load the model with the local path\n",
    "# model = lp.Detectron2LayoutModel(\n",
    "#     config_path='model/config.yaml',\n",
    "#     model_path='model/model_final.pth',\n",
    "#     label_map={0: \"Text\", 1: \"Title\", 2: \"List\", 3: \"Table\", 4: \"Figure\"},\n",
    "#     extra_config=[\"MODEL.ROI_HEADS.SCORE_THRESH_TEST\", 0.8]\n",
    "# )\n",
    "\n",
    "import layoutparser as lp\n",
    "model = lp.Detectron2LayoutModel(\n",
    "            config_path ='lp://PubLayNet/mask_rcnn_X_101_32x8d_FPN_3x/config', # In model catalog\n",
    "            label_map   ={0: \"Text\", 1: \"Title\", 2: \"List\", 3:\"Table\", 4:\"Figure\"}, # In model`label_map`\n",
    "            extra_config=[\"MODEL.ROI_HEADS.SCORE_THRESH_TEST\", 0.8] # Optional\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF\n",
    "import os\n",
    "import layoutparser as lp\n",
    "import cv2\n",
    "import pytesseract\n",
    "from pytesseract import Output\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import re\n",
    "import numpy as np\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from fuzzywuzzy import fuzz\n",
    "import logging\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Convert PDF pages to images\n",
    "def render_pdf_pages_to_images(pdf_path, image_folder, zoom=2):\n",
    "    try:\n",
    "        if not os.path.exists(image_folder):\n",
    "            os.makedirs(image_folder)\n",
    "\n",
    "        document = fitz.open(pdf_path)\n",
    "        for page_number, page in enumerate(document):\n",
    "            mat = fitz.Matrix(zoom, zoom)\n",
    "            pix = page.get_pixmap(matrix=mat)\n",
    "            image_filename = f\"{image_folder}/output_page_{page_number + 1}.png\"\n",
    "            pix.save(image_filename)\n",
    "            logging.info(f\"Saved {image_filename}\")\n",
    "\n",
    "        document.close()\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error rendering PDF pages to images: {e}\")\n",
    "\n",
    "# Combine font information extracted by PyMuPDF and Tesseract\n",
    "def extract_combined_font_info(page, image_path):\n",
    "    try:\n",
    "        # Extract font info using PyMuPDF\n",
    "        font_info_pymupdf = []\n",
    "        blocks = page.get_text(\"dict\")[\"blocks\"]\n",
    "        for block in blocks:\n",
    "            if \"lines\" in block:\n",
    "                for line in block[\"lines\"]:\n",
    "                    for span in line[\"spans\"]:\n",
    "                        font_info_pymupdf.append({\n",
    "                            \"bold\": span[\"flags\"] & 2 != 0,\n",
    "                            \"italic\": span[\"flags\"] & 1 != 0,\n",
    "                            \"size\": span[\"size\"],\n",
    "                            \"text\": span[\"text\"]\n",
    "                        })\n",
    "        \n",
    "        # Extract font info using Tesseract\n",
    "        img = cv2.imread(image_path)\n",
    "        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        data = pytesseract.image_to_data(img_rgb, output_type=Output.DICT)\n",
    "        \n",
    "        # Check if the necessary keys are in the Tesseract output\n",
    "        required_keys = ['level', 'text', 'height']\n",
    "        for key in required_keys:\n",
    "            if key not in data:\n",
    "                logging.error(f\"Key '{key}' not found in Tesseract output\")\n",
    "                return []\n",
    "\n",
    "        font_info_tesseract = []\n",
    "        for i in range(len(data['level'])):\n",
    "            text = data['text'][i].strip()\n",
    "            if text:\n",
    "                font_info_tesseract.append({\n",
    "                    \"bold\": 'Bold' in data.get('font', [''])[i] if 'font' in data else False,\n",
    "                    \"italic\": 'Italic' in data.get('font', [''])[i] if 'font' in data else False,\n",
    "                    \"size\": int(data['height'][i]),\n",
    "                    \"text\": text\n",
    "                })\n",
    "        \n",
    "        # Combine results\n",
    "        combined_font_info = []\n",
    "        for pymupdf_info in font_info_pymupdf:\n",
    "            for tess_info in font_info_tesseract:\n",
    "                if fuzz.ratio(pymupdf_info['text'], tess_info['text']) > 80:\n",
    "                    combined_info = {\n",
    "                        \"bold\": pymupdf_info[\"bold\"] or tess_info[\"bold\"],\n",
    "                        \"italic\": pymupdf_info[\"italic\"] or tess_info[\"italic\"],\n",
    "                        \"size\": max(pymupdf_info[\"size\"], tess_info[\"size\"]),\n",
    "                        \"text\": pymupdf_info[\"text\"]\n",
    "                    }\n",
    "                    combined_font_info.append(combined_info)\n",
    "                    break\n",
    "        return combined_font_info\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error extracting combined font info: {e}\")\n",
    "        return []\n",
    "\n",
    "# Check if a block is a potential caption\n",
    "def is_potential_caption(block, figure_blocks, distance=20, word_limit=50):\n",
    "    text = block.text\n",
    "    if \"Â©\" in text:\n",
    "        return True\n",
    "\n",
    "    title_keywords = [\"funding\", \"conflict of interest\", \"supplementary materials\", \"declaration\",\n",
    "                      \"acknowledgments\", \"data availability\", \"author contributions\", \"publisher's note\", \"appendix\"]\n",
    "\n",
    "    if block.type == 'Title':\n",
    "        for keyword in title_keywords:\n",
    "            if fuzz.partial_ratio(text.lower(), keyword) > 50:\n",
    "                return False\n",
    "\n",
    "    if re.match(r'^(Fig\\.|Figure|Table|Box)\\s*\\d+|^(supplementary\\w*|appendix\\w*|fund\\w*|conflict of interest\\w*|data availability\\w*|publi\\w*|abbreviation\\w*|author\\w*|the author|copyright|correspond\\w*|assess\\w*|email\\w*|tel\\w*|open access|keywords|key words|address\\w*|receive\\w*|review\\w*)', text, re.IGNORECASE):\n",
    "        return True\n",
    "\n",
    "    for figure_block in figure_blocks:\n",
    "        x0, y0, x1, y1 = figure_block.coordinates\n",
    "        bx0, by0, bx1, by1 = block.coordinates\n",
    "\n",
    "        if (\n",
    "            abs(y0 - by1) <= distance or abs(y1 - by0) <= distance or\n",
    "            abs(x0 - bx1) <= distance or abs(x1 - bx0) <= distance\n",
    "        ):\n",
    "            if len(block.text.strip().split()) < word_limit:\n",
    "                return True\n",
    "    return False \n",
    "\n",
    "# Sort blocks by coordinates with a tolerance of 50\n",
    "TOLERANCE = 50\n",
    "\n",
    "def sort_blocks_by_coordinates(blocks, tolerance=TOLERANCE):\n",
    "    if not blocks:\n",
    "        return []\n",
    "\n",
    "    blocks = sorted(blocks, key=lambda b: b.coordinates[0])\n",
    "    \n",
    "    sorted_blocks = []\n",
    "    current_line = []\n",
    "    current_x = blocks[0].coordinates[0]\n",
    "    \n",
    "    for block in blocks:\n",
    "        if abs(block.coordinates[0] - current_x) <= tolerance:\n",
    "            current_line.append(block)\n",
    "        else:\n",
    "            current_line.sort(key=lambda b: b.coordinates[1])\n",
    "            sorted_blocks.extend(current_line)\n",
    "            current_line = [block]\n",
    "            current_x = block.coordinates[0]\n",
    "    \n",
    "    current_line.sort(key=lambda b: b.coordinates[1])\n",
    "    sorted_blocks.extend(current_line)\n",
    "    \n",
    "    return sorted_blocks\n",
    "\n",
    "def draw_box(image, layout, box_width=3, show_element_id=True):\n",
    "    pil_image = Image.fromarray(image)\n",
    "    draw = ImageDraw.Draw(pil_image)\n",
    "\n",
    "    for element in layout:\n",
    "        box = element.coordinates\n",
    "        draw.rectangle(box, outline=\"red\", width=box_width)\n",
    "\n",
    "        if show_element_id and hasattr(element, 'id'):\n",
    "            font = ImageFont.load_default()\n",
    "            text = f\"ID: {element.id}\"\n",
    "            text_bbox = font.getbbox(text)\n",
    "            text_width = text_bbox[2] - text_bbox[0]\n",
    "            text_height = text_bbox[3] - text_bbox[1]\n",
    "            draw.rectangle([box[0], box[1] - text_height, box[0] + text_width, box[1]], fill=\"red\")\n",
    "            draw.text((box[0], box[1] - text_height), text, fill=\"white\", font=font)\n",
    "\n",
    "    image = cv2.cvtColor(np.array(pil_image), cv2.COLOR_RGB2BGR)\n",
    "    return image\n",
    "\n",
    "# Modified function to extract abstract from the first pages\n",
    "def extract_abstract_from_first_pages(images, model, output_folder):\n",
    "    try:\n",
    "        ocr_agent = lp.TesseractAgent(languages='eng')\n",
    "        abstract_text = \"\"\n",
    "        first_title = None\n",
    "        abstract_block_info = []\n",
    "        \n",
    "        for page_number, image in enumerate(images):\n",
    "            layout = model.detect(image)\n",
    "            if layout is None:\n",
    "                raise ValueError(\"Model detection returned None\")\n",
    "\n",
    "            for idx, block in enumerate(layout):\n",
    "                block.id = idx\n",
    "                segment_image = block.pad(left=5, right=5, top=5, bottom=5).crop_image(image)\n",
    "                text = ocr_agent.detect(segment_image).strip()\n",
    "                block.set(text=text, inplace=True)\n",
    "            \n",
    "            abstract_block = None\n",
    "\n",
    "            # Step 1: Look for an \"Abstract\" title block\n",
    "            for block in layout:\n",
    "                if block.type == 'Title' and block.text.strip().lower() == \"abstract\":\n",
    "                    abstract_block = block\n",
    "                    break\n",
    "\n",
    "            if abstract_block and abstract_block.id is not None:\n",
    "                # Collect text blocks below the \"Abstract\" title block\n",
    "                for block in layout:\n",
    "                    if block.id is not None and block.id > abstract_block.id:\n",
    "                        if block.type == 'Title' and block.text.strip().lower() == \"introduction\":\n",
    "                            break  # Stop at the \"Introduction\" title block\n",
    "                        abstract_text += block.text.strip() + \" \"\n",
    "                        abstract_block_info.append({\n",
    "                            \"Page Name\": f\"Page {page_number + 1}\",\n",
    "                            \"Block ID\": block.id,\n",
    "                            \"Text\": block.text.strip()\n",
    "                        })\n",
    "                        if len(block.text.strip()) == 0:\n",
    "                            break\n",
    "\n",
    "            if not abstract_text.strip():\n",
    "                # Step 2: Keyword matching\n",
    "                keywords = [\"aim\", \"aims\", \"background\", \"purpose\", \"purposes\", \"introduction\", \"objective\", \"objectives\", \n",
    "                            \"method\", \"methods\", \"material\",\"materials\", \"materials and methods\", \"introduction\",\n",
    "                            \"result\",\"results\", \"conclusion\", \"conclusions\", \"discussion\", \"areas covered\", \"expert opinion\", \"background and objectives\"]\n",
    "                keyword_blocks = []\n",
    "                \n",
    "                for block in layout:\n",
    "                    text = block.text.strip().lower()\n",
    "                    if any(fuzz.partial_ratio(text, keyword) > 80 for keyword in keywords):\n",
    "                        keyword_blocks.append(block)\n",
    "                \n",
    "                if keyword_blocks:\n",
    "                    # Sort keyword blocks by y-coordinate and collect nearby blocks within 20 pixels\n",
    "                    keyword_blocks = sorted(keyword_blocks, key=lambda b: b.coordinates[1])\n",
    "                    for keyword_block in keyword_blocks:\n",
    "                        for block in layout:\n",
    "                            if abs(block.coordinates[1] - keyword_block.coordinates[1]) <= 20:\n",
    "                                abstract_text += block.text.strip() + \" \"\n",
    "                                abstract_block_info.append({\n",
    "                                    \"Page Name\": f\"Page {page_number + 1}\",\n",
    "                                    \"Block ID\": block.id,\n",
    "                                    \"Text\": block.text.strip()\n",
    "                                })\n",
    "\n",
    "            if not first_title:\n",
    "                # Look for the first title that is not \"Abstract\"\n",
    "                for block in layout:\n",
    "                    if block.type == 'Title' and block.text.strip().lower() != \"abstract\":\n",
    "                        first_title = block.text.strip().replace('\\n', ' ')\n",
    "                        break\n",
    "\n",
    "        # Remove the word \"abstract\" from the abstract text\n",
    "        abstract_text = abstract_text.replace(\"abstract\", \"\").strip()\n",
    "\n",
    "        if not abstract_text.strip():\n",
    "            # Step 3: Heuristic method\n",
    "            text_blocks = [block.text.strip() for block in layout if block.type == 'Text']\n",
    "            for text in text_blocks[:5]:  # Assuming the abstract appears in the first five blocks\n",
    "                if len(text.split()) > 50:\n",
    "                    abstract_text = text\n",
    "                    break\n",
    "\n",
    "        # Save abstract block information to a CSV file\n",
    "        df_abstract_info = pd.DataFrame(abstract_block_info)\n",
    "        df_abstract_info.to_csv(os.path.join(output_folder, 'abstract_info.csv'), index=False)\n",
    "\n",
    "        return abstract_text.strip(), first_title\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error extracting abstract: {e}\")\n",
    "        return \"\", None\n",
    "\n",
    "# Generate a table of title information\n",
    "def generate_title_info_table(sorted_blocks, font_info_dict, output_folder):\n",
    "    title_data = []\n",
    "    for block in sorted_blocks:\n",
    "        if block.type == 'Title':\n",
    "            page_number = block.page_number\n",
    "            text = block.text.strip()\n",
    "            font_info = next((info for info in font_info_dict[page_number] if info['text'] in text), None)\n",
    "            title_data.append({\n",
    "                \"Text\": text,\n",
    "                \"Page Number\": page_number,\n",
    "                \"Block ID\": block.id,\n",
    "                \"Bold\": font_info.get(\"bold\", False) if font_info else False,\n",
    "                \"Italic\": font_info.get(\"italic\", False) if font_info else False,\n",
    "                \"Size\": font_info.get(\"size\", 0) if font_info else 0\n",
    "            })\n",
    "\n",
    "    df = pd.DataFrame(title_data)\n",
    "    df = df.sort_values(by=[\"Page Number\", \"Block ID\"]).reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "def filter_useless_title_blocks(df, title_keywords, output_folder):\n",
    "    df_filtered = df[~df['Text'].apply(lambda x: any(fuzz.partial_ratio(x.strip().lower(), keyword) > 90 for keyword in title_keywords))]\n",
    "    df_filtered.to_csv(os.path.join(output_folder, 'useful_title_block.csv'), index=False)\n",
    "    return df_filtered\n",
    "\n",
    "# Find reference title\n",
    "def find_reference_title(df):\n",
    "    reference_titles = [\"introduction\", \"background\", \"conclusion\",\"conclusions\", \"references\"]\n",
    "    for title in reference_titles:\n",
    "        for index, row in df.iterrows():\n",
    "            if title in row['Text'].lower():\n",
    "                logging.info(f\"Reference title found: {row['Text']}\")\n",
    "                return row\n",
    "    return None\n",
    "\n",
    "# Update title info table and mark similar titles\n",
    "def mark_similar_titles(df, reference_title, paper_title):\n",
    "    try:\n",
    "        def is_similar_row(row1, row2):\n",
    "            return (\n",
    "                row1['Bold'] == row2['Bold'] and\n",
    "                row1['Italic'] == row2['Italic'] and\n",
    "                abs(row1['Size'] - row2['Size']) <= 1\n",
    "            )\n",
    "\n",
    "        if reference_title is not None:\n",
    "            df['Similar'] = df.apply(lambda x: \"title\" if x['Text'] == paper_title else (\"Ref\" if x.equals(reference_title) else is_similar_row(x, reference_title)), axis=1)\n",
    "        else:\n",
    "            df['Similar'] = df.apply(lambda x: \"title\" if x['Text'] == paper_title else False, axis=1)\n",
    "\n",
    "        for index, row in df.iterrows():\n",
    "            if row['Similar'] != False:\n",
    "                logging.info(f\"Row {index}: {row['Text']} | Bold: {row['Bold']} | Italic: {row['Italic']} | Size: {row['Size']} | Similar: {row['Similar']}\")\n",
    "\n",
    "        for index, row in df.iterrows():\n",
    "            if row['Similar'] != False:\n",
    "                logging.info(f\"Section heading found: {row['Text']}\")\n",
    "\n",
    "        df['New ID'] = df.index + 1\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error marking similar titles: {e}\")\n",
    "        return df\n",
    "\n",
    "# Extract text from images\n",
    "def extract_text_from_images(image_folder, output_folder, model, pdf_path):\n",
    "    try:\n",
    "        ocr_agent = lp.TesseractAgent(languages='eng')\n",
    "        all_text_blocks = []\n",
    "        all_captions = []\n",
    "        font_info_dict = {}\n",
    "        abstract_text = None\n",
    "        first_title = None\n",
    "        first_page_processed = False\n",
    "\n",
    "        # Extract abstract and title\n",
    "        if not first_page_processed:\n",
    "            first_image_path = os.path.join(image_folder, 'output_page_1.png')\n",
    "            second_image_path = os.path.join(image_folder, 'output_page_2.png')\n",
    "            images = [cv2.imread(first_image_path)[..., ::-1], cv2.imread(second_image_path)[..., ::-1]]\n",
    "            abstract_text, first_title = extract_abstract_from_first_pages(images, model, output_folder)\n",
    "            first_page_processed = True\n",
    "\n",
    "        # Load PDF once\n",
    "        document = fitz.open(pdf_path)\n",
    "        import re\n",
    "\n",
    "        def natural_sort_key(s, _nsre=re.compile('([0-9]+)')):\n",
    "            return [int(text) if text.isdigit() else text.lower() for text in re.split(_nsre, s)]\n",
    "\n",
    "        sorted_image_files = sorted(os.listdir(image_folder), key=natural_sort_key)\n",
    "\n",
    "        def process_page(page_number, image_name):\n",
    "            try:\n",
    "                image_path = os.path.join(image_folder, image_name)\n",
    "                image = cv2.imread(image_path)\n",
    "                image = image[..., ::-1]\n",
    "\n",
    "                page = document.load_page(page_number - 1)\n",
    "                font_info = extract_combined_font_info(page, image_path)\n",
    "                if not font_info:\n",
    "                    logging.warning(f\"Font info extraction failed for page {page_number}\")\n",
    "\n",
    "                layout = model.detect(image)\n",
    "                if layout is None:\n",
    "                    raise ValueError(\"Model detection returned None\")\n",
    "\n",
    "                for block in layout:\n",
    "                    segment_image = block.pad(left=5, right=5, top=5, bottom=5).crop_image(image)\n",
    "                    text = ocr_agent.detect(segment_image)\n",
    "                    block.set(text=text, inplace=True)\n",
    "                    block.page_number = page_number\n",
    "\n",
    "                text_blocks = lp.Layout([b for b in layout if b.type in ['Text', 'Title', 'List']])\n",
    "                figure_blocks = lp.Layout([b for b in layout if b.type in ['Figure', 'Table']])\n",
    "                text_blocks = lp.Layout([b for b in text_blocks if not any(b.is_in(b_fig) for b_fig in figure_blocks)])\n",
    "                \n",
    "                sorted_blocks = sort_blocks_by_coordinates(text_blocks)\n",
    "\n",
    "                if sorted_blocks:  # Check if empty\n",
    "                    for idx, block in enumerate(sorted_blocks):\n",
    "                        block.id = idx  # Directly set id attribute\n",
    "\n",
    "                captions = []\n",
    "                filtered_text_blocks = []\n",
    "\n",
    "                for block in sorted_blocks:\n",
    "                    if is_potential_caption(block, figure_blocks):\n",
    "                        captions.append(block)\n",
    "                    else:\n",
    "                        filtered_text_blocks.append(block)\n",
    "\n",
    "                image_with_boxes = draw_box(image, sorted_blocks)\n",
    "                output_image_filename = os.path.join(output_folder, 'images_with_boxes', f\"{os.path.splitext(image_name)[0]}_with_boxes.png\")\n",
    "                cv2.imwrite(output_image_filename, image_with_boxes)\n",
    "\n",
    "                return filtered_text_blocks, captions, font_info\n",
    "\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error processing page {page_number}: {e}\")\n",
    "                return [], [], []\n",
    "\n",
    "        # Use multithreading to process each page in parallel\n",
    "        with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "            futures = [executor.submit(process_page, int(re.search(r'\\d+', image_name).group()), image_name) for image_name in sorted_image_files]\n",
    "            for future in as_completed(futures):\n",
    "                filtered_text_blocks, captions, font_info = future.result()\n",
    "                all_text_blocks.extend(filtered_text_blocks)\n",
    "                all_captions.extend(captions)\n",
    "                if font_info:\n",
    "                    page_number = filtered_text_blocks[0].page_number if filtered_text_blocks else captions[0].page_number\n",
    "                    font_info_dict[page_number] = font_info\n",
    "\n",
    "        os.makedirs(os.path.join(output_folder, 'text'), exist_ok=True)\n",
    "        os.makedirs(os.path.join(output_folder, 'pickle'), exist_ok=True)\n",
    "\n",
    "        with open(os.path.join(output_folder, 'text', 'captions.txt'), 'w', encoding='utf-8') as caption_file:\n",
    "            for caption in all_captions:\n",
    "                caption_file.write(caption.text + \"\\n\")\n",
    "        \n",
    "        all_text_blocks.sort(key=lambda b: (b.page_number, b.id))\n",
    "\n",
    "        if abstract_text:\n",
    "            with open(os.path.join(output_folder, 'text', 'Abstract.txt'), 'w', encoding='utf-8') as abstract_file:\n",
    "                abstract_file.write(abstract_text)\n",
    "            with open(os.path.join(output_folder, 'pickle', 'Abstract.pkl'), 'wb') as abstract_pkl_file:\n",
    "                pickle.dump(abstract_text, abstract_pkl_file)\n",
    "\n",
    "        return all_text_blocks, font_info_dict, abstract_text, first_title\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error extracting text from images: {e}\")\n",
    "        return [], {}, \"\", None\n",
    "\n",
    "# Extract and save sections\n",
    "def extract_and_save_sections(sorted_blocks, df, output_folder):\n",
    "    sections = {}\n",
    "    current_section = None\n",
    "\n",
    "    for block in sorted_blocks:\n",
    "        if block.type == 'Title':\n",
    "            text = block.text.strip()\n",
    "            if re.match(r'^\\d+(\\.\\d+)?', text):\n",
    "                if re.match(r'^\\d+\\.\\d+', text):\n",
    "                    if current_section:\n",
    "                        sections[current_section].append(text)\n",
    "                else:\n",
    "                    current_section = text\n",
    "                    sections[current_section] = []\n",
    "            elif text.lower() in df[df['Similar'].isin([True, 'Ref'])]['Text'].str.lower().tolist():\n",
    "                current_section = text\n",
    "                sections[current_section] = []\n",
    "\n",
    "        if current_section and block.type != 'Title':\n",
    "            sections[current_section].append(block.text.strip())\n",
    "\n",
    "    section_order = []\n",
    "    for section, texts in sections.items():\n",
    "        section_text = \" \".join(texts).replace('\\n', ' ')\n",
    "        filename = f\"{section}.pkl\".replace(\" \", \"_\").replace(\"/\", \"_\")\n",
    "        section_order.append((section, filename))\n",
    "        with open(os.path.join(output_folder, 'pickle', filename), 'wb') as f:\n",
    "            pickle.dump(section_text, f)\n",
    "        with open(os.path.join(output_folder, 'text', f\"{section}.txt\".replace(\" \", \"_\").replace(\"/\", \"_\")), 'w', encoding='utf-8') as section_file:\n",
    "            section_file.write(section_text)\n",
    "        logging.info(f\"Saved {section} to {filename}\")\n",
    "\n",
    "    return section_order\n",
    "\n",
    "# Generate CSV file\n",
    "def generate_csv(output_root_folder, df_data):\n",
    "    rows = []\n",
    "    for data in df_data:\n",
    "        row = {'PDF name': data['pdf_name'], 'article': data['paper_title'], 'abstract': data['abstract']}\n",
    "        for idx, (section, filename) in enumerate(data['section_order'], start=1):\n",
    "            if filename == 'Abstract.pkl':\n",
    "                continue\n",
    "            row[f'section {idx}'] = filename\n",
    "        rows.append(row)\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    df.to_csv(os.path.join(output_root_folder, 'all_papers.csv'), index=False)\n",
    "\n",
    "# Process all PDFs in a folder\n",
    "def process_pdfs_in_folder(pdf_folder, output_root_folder, model):\n",
    "    try:\n",
    "        df_data = []\n",
    "\n",
    "        for pdf_file in os.listdir(pdf_folder):\n",
    "            if pdf_file.endswith('.pdf'):\n",
    "                pdf_path = os.path.join(pdf_folder, pdf_file)\n",
    "                pdf_name = os.path.splitext(pdf_file)[0]\n",
    "                pdf_output_folder = os.path.join(output_root_folder, pdf_name)\n",
    "\n",
    "                for folder in ['images', 'images_with_boxes', 'text', 'pickle']:\n",
    "                    os.makedirs(os.path.join(pdf_output_folder, folder), exist_ok=True)\n",
    "\n",
    "                render_pdf_pages_to_images(pdf_path, os.path.join(pdf_output_folder, 'images'))\n",
    "\n",
    "                all_text_blocks, font_info_dict, abstract_text, first_title = extract_text_from_images(os.path.join(pdf_output_folder, 'images'), pdf_output_folder, model, pdf_path)\n",
    "                \n",
    "                df = generate_title_info_table(all_text_blocks, font_info_dict, pdf_output_folder)\n",
    "\n",
    "                reference_title = find_reference_title(df)\n",
    "\n",
    "                df = mark_similar_titles(df, reference_title, first_title)\n",
    "                df.to_csv(os.path.join(pdf_output_folder, 'title_blocks_info.csv'), index=False)\n",
    "\n",
    "                section_order = extract_and_save_sections(all_text_blocks, df, pdf_output_folder)\n",
    "                \n",
    "                # Sort section_order by New ID in df\n",
    "                sorted_section_order = sorted(section_order, key=lambda x: df[df['Text'] == x[0]]['New ID'].values[0] if x[0] in df['Text'].values else float('inf'))\n",
    "\n",
    "                df_data.append({\n",
    "                    'pdf_name': pdf_name,\n",
    "                    'paper_title': first_title,\n",
    "                    'abstract': 'Abstract.pkl',\n",
    "                    'section_order': sorted_section_order\n",
    "                })\n",
    "\n",
    "        generate_csv(output_root_folder, df_data)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing PDFs in folder: {e}\")\n",
    "\n",
    "# Usage example\n",
    "pdf_folder = 'Diabetes PDFs'\n",
    "output_root_folder = 'Diabetes PDFs Outputs'\n",
    "\n",
    "process_pdfs_in_folder(pdf_folder, output_root_folder, model)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import layoutparser as lp\n",
    "import cv2\n",
    "\n",
    "# Load the model with the local path\n",
    "model = lp.Detectron2LayoutModel(\n",
    "    config_path='model/config.yaml',\n",
    "    model_path='model/model_final.pth',\n",
    "    label_map={0: \"Text\", 1: \"Title\", 2: \"List\", 3: \"Table\", 4: \"Figure\"},\n",
    "    extra_config=[\"MODEL.ROI_HEADS.SCORE_THRESH_TEST\", 0.8]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF\n",
    "import os\n",
    "import layoutparser as lp\n",
    "import cv2\n",
    "import pytesseract\n",
    "from pytesseract import Output\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import re\n",
    "import numpy as np\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from fuzzywuzzy import fuzz\n",
    "import logging\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "# 设置日志\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# 将PDF页面转换为图像\n",
    "def render_pdf_pages_to_images(pdf_path, image_folder, zoom=2):\n",
    "    try:\n",
    "        if not os.path.exists(image_folder):\n",
    "            os.makedirs(image_folder)\n",
    "\n",
    "        document = fitz.open(pdf_path)\n",
    "        for page_number, page in enumerate(document):\n",
    "            mat = fitz.Matrix(zoom, zoom)\n",
    "            pix = page.get_pixmap(matrix=mat)\n",
    "            image_filename = f\"{image_folder}/output_page_{page_number + 1}.png\"\n",
    "            pix.save(image_filename)\n",
    "            logging.info(f\"Saved {image_filename}\")\n",
    "\n",
    "        document.close()\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error rendering PDF pages to images: {e}\")\n",
    "\n",
    "# 合并PyMuPDF和Tesseract提取的字体信息\n",
    "def extract_combined_font_info(page, image_path):\n",
    "    try:\n",
    "        # Extract font info using PyMuPDF\n",
    "        font_info_pymupdf = []\n",
    "        blocks = page.get_text(\"dict\")[\"blocks\"]\n",
    "        for block in blocks:\n",
    "            if \"lines\" in block:\n",
    "                for line in block[\"lines\"]:\n",
    "                    for span in line[\"spans\"]:\n",
    "                        font_info_pymupdf.append({\n",
    "                            \"bold\": span[\"flags\"] & 2 != 0,\n",
    "                            \"italic\": span[\"flags\"] & 1 != 0,\n",
    "                            \"size\": span[\"size\"],\n",
    "                            \"text\": span[\"text\"]\n",
    "                        })\n",
    "        \n",
    "        # Extract font info using Tesseract\n",
    "        img = cv2.imread(image_path)\n",
    "        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        data = pytesseract.image_to_data(img_rgb, output_type=Output.DICT)\n",
    "        \n",
    "        # Check if the necessary keys are in the Tesseract output\n",
    "        required_keys = ['level', 'text', 'height']\n",
    "        for key in required_keys:\n",
    "            if key not in data:\n",
    "                logging.error(f\"Key '{key}' not found in Tesseract output\")\n",
    "                return []\n",
    "\n",
    "        font_info_tesseract = []\n",
    "        for i in range(len(data['level'])):\n",
    "            text = data['text'][i].strip()\n",
    "            if text:\n",
    "                font_info_tesseract.append({\n",
    "                    \"bold\": 'Bold' in data.get('font', [''])[i] if 'font' in data else False,\n",
    "                    \"italic\": 'Italic' in data.get('font', [''])[i] if 'font' in data else False,\n",
    "                    \"size\": int(data['height'][i]),\n",
    "                    \"text\": text\n",
    "                })\n",
    "        \n",
    "        # Combine results\n",
    "        combined_font_info = []\n",
    "        for pymupdf_info in font_info_pymupdf:\n",
    "            for tess_info in font_info_tesseract:\n",
    "                if fuzz.ratio(pymupdf_info['text'], tess_info['text']) > 80:\n",
    "                    combined_info = {\n",
    "                        \"bold\": pymupdf_info[\"bold\"] or tess_info[\"bold\"],\n",
    "                        \"italic\": pymupdf_info[\"italic\"] or tess_info[\"italic\"],\n",
    "                        \"size\": max(pymupdf_info[\"size\"], tess_info[\"size\"]),\n",
    "                        \"text\": pymupdf_info[\"text\"]\n",
    "                    }\n",
    "                    combined_font_info.append(combined_info)\n",
    "                    break\n",
    "        return combined_font_info\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error extracting combined font info: {e}\")\n",
    "        return []\n",
    "\n",
    "# 检查是否是可能的图注\n",
    "def is_potential_caption(block, figure_blocks, distance=20, word_limit=50):\n",
    "    text = block.text\n",
    "    if \"©\" in text:\n",
    "        return True\n",
    "\n",
    "    title_keywords = [\"funding\", \"conflict of interest\", \"supplementary materials\", \"declaration\",\n",
    "                      \"acknowledgments\", \"Data availability\", \"Author contributions\", \"Publisher's note\", \"appendix\"]\n",
    "\n",
    "    if block.type == 'Title':\n",
    "        for keyword in title_keywords:\n",
    "            if fuzz.partial_ratio(text.lower(), keyword) > 50:\n",
    "                return False\n",
    "\n",
    "    if re.match(r'^(Fig\\.|Figure|Table|Box)\\s*\\d+|^(supplementary\\w*|appendix\\w*|fund\\w*|conflict of interest\\w*|Data availability\\w*|Publi\\w*|Abbreviation\\w*|Author\\w*|The Author|Copyright|Correspond\\w*|Assess\\w*|Email\\w*|Tel\\w*|Open access|Keywords|Key words|Address\\w*|Receive\\w*|Review\\w*)', text, re.IGNORECASE):\n",
    "        return True\n",
    "\n",
    "    for figure_block in figure_blocks:\n",
    "        x0, y0, x1, y1 = figure_block.coordinates\n",
    "        bx0, by0, bx1, by1 = block.coordinates\n",
    "\n",
    "        if (\n",
    "            abs(y0 - by1) <= distance or abs(y1 - by0) <= distance or\n",
    "            abs(x0 - bx1) <= distance or abs(x1 - bx0) <= distance\n",
    "        ):\n",
    "            if len(block.text.strip().split()) < word_limit:\n",
    "                return True\n",
    "    return False \n",
    "\n",
    "# 按坐标排序块，容忍度为50\n",
    "TOLERANCE = 50\n",
    "\n",
    "def sort_blocks_by_coordinates(blocks, tolerance=TOLERANCE):\n",
    "    if not blocks:\n",
    "        return []\n",
    "\n",
    "    blocks = sorted(blocks, key=lambda b: b.coordinates[0])\n",
    "    \n",
    "    sorted_blocks = []\n",
    "    current_line = []\n",
    "    current_x = blocks[0].coordinates[0]\n",
    "    \n",
    "    for block in blocks:\n",
    "        if abs(block.coordinates[0] - current_x) <= tolerance:\n",
    "            current_line.append(block)\n",
    "        else:\n",
    "            current_line.sort(key=lambda b: b.coordinates[1])\n",
    "            sorted_blocks.extend(current_line)\n",
    "            current_line = [block]\n",
    "            current_x = block.coordinates[0]\n",
    "    \n",
    "    current_line.sort(key=lambda b: b.coordinates[1])\n",
    "    sorted_blocks.extend(current_line)\n",
    "    \n",
    "    return sorted_blocks\n",
    "\n",
    "def draw_box(image, layout, box_width=3, show_element_id=True):\n",
    "    pil_image = Image.fromarray(image)\n",
    "    draw = ImageDraw.Draw(pil_image)\n",
    "\n",
    "    for element in layout:\n",
    "        box = element.coordinates\n",
    "        draw.rectangle(box, outline=\"red\", width=box_width)\n",
    "\n",
    "        if show_element_id and hasattr(element, 'id'):\n",
    "            font = ImageFont.load_default()\n",
    "            text = f\"ID: {element.id}\"\n",
    "            text_bbox = font.getbbox(text)\n",
    "            text_width = text_bbox[2] - text_bbox[0]\n",
    "            text_height = text_bbox[3] - text_bbox[1]\n",
    "            draw.rectangle([box[0], box[1] - text_height, box[0] + text_width, box[1]], fill=\"red\")\n",
    "            draw.text((box[0], box[1] - text_height), text, fill=\"white\", font=font)\n",
    "\n",
    "    image = cv2.cvtColor(np.array(pil_image), cv2.COLOR_RGB2BGR)\n",
    "    return image\n",
    "\n",
    "# 修改后的extract_abstract_from_first_pages函数\n",
    "def extract_abstract_from_first_pages(images, model, output_folder):\n",
    "    try:\n",
    "        ocr_agent = lp.TesseractAgent(languages='eng')\n",
    "        abstract_text = \"\"\n",
    "        first_title = None\n",
    "        abstract_block_info = []\n",
    "        \n",
    "        for page_number, image in enumerate(images):\n",
    "            layout = model.detect(image)\n",
    "            if layout is None:\n",
    "                raise ValueError(\"Model detection returned None\")\n",
    "\n",
    "            for idx, block in enumerate(layout):\n",
    "                block.id = idx\n",
    "                segment_image = block.pad(left=5, right=5, top=5, bottom=5).crop_image(image)\n",
    "                text = ocr_agent.detect(segment_image).strip()\n",
    "                block.set(text=text, inplace=True)\n",
    "            \n",
    "            abstract_block = None\n",
    "\n",
    "            # Step 1: Look for an \"Abstract\" title block\n",
    "            for block in layout:\n",
    "                if block.type == 'Title' and block.text.strip().lower() == \"abstract\":\n",
    "                    abstract_block = block\n",
    "                    break\n",
    "\n",
    "            if abstract_block and abstract_block.id is not None:\n",
    "                # Collect text blocks below the \"Abstract\" title block\n",
    "                for block in layout:\n",
    "                    if block.id is not None and block.id > abstract_block.id:\n",
    "                        if block.type == 'Title' and block.text.strip().lower() == \"introduction\":\n",
    "                            break  # Stop at the \"Introduction\" title block\n",
    "                        abstract_text += block.text.strip() + \" \"\n",
    "                        abstract_block_info.append({\n",
    "                            \"Page Name\": f\"Page {page_number + 1}\",\n",
    "                            \"Block ID\": block.id,\n",
    "                            \"Text\": block.text.strip()\n",
    "                        })\n",
    "                        if len(block.text.strip()) == 0:\n",
    "                            break\n",
    "\n",
    "            if not abstract_text.strip():\n",
    "                # Step 2: Keyword matching\n",
    "                keywords = [\"aim\", \"aims\", \"background\", \"purpose\", \"purposes\", \"introduction\", \"objective\", \"objectives\", \n",
    "                            \"method\", \"methods\", \"material\",\"materials\", \"materials and methods\", \"introduction\",\n",
    "                            \"result\",\"results\", \"conclusion\", \"conclusions\", \"discussion\", \"Areas covered\", \"expert opinion\", \"background and objectives\"]\n",
    "                keyword_blocks = []\n",
    "                \n",
    "                for block in layout:\n",
    "                    text = block.text.strip().lower()\n",
    "                    if any(fuzz.partial_ratio(text, keyword) > 80 for keyword in keywords):\n",
    "                        keyword_blocks.append(block)\n",
    "                \n",
    "                if keyword_blocks:\n",
    "                    # Sort keyword blocks by y-coordinate and collect nearby blocks within 20 pixels\n",
    "                    keyword_blocks = sorted(keyword_blocks, key=lambda b: b.coordinates[1])\n",
    "                    for keyword_block in keyword_blocks:\n",
    "                        for block in layout:\n",
    "                            if abs(block.coordinates[1] - keyword_block.coordinates[1]) <= 20:\n",
    "                                abstract_text += block.text.strip() + \" \"\n",
    "                                abstract_block_info.append({\n",
    "                                    \"Page Name\": f\"Page {page_number + 1}\",\n",
    "                                    \"Block ID\": block.id,\n",
    "                                    \"Text\": block.text.strip()\n",
    "                                })\n",
    "\n",
    "            if not first_title:\n",
    "                # Look for the first title that is not \"Abstract\"\n",
    "                for block in layout:\n",
    "                    if block.type == 'Title' and block.text.strip().lower() != \"abstract\":\n",
    "                        first_title = block.text.strip().replace('\\n', ' ')\n",
    "                        break\n",
    "\n",
    "        # 移除摘要文本中的“abstract”字符\n",
    "        abstract_text = abstract_text.replace(\"abstract\", \"\").strip()\n",
    "\n",
    "        if not abstract_text.strip():\n",
    "            # Step 3: Heuristic method\n",
    "            text_blocks = [block.text.strip() for block in layout if block.type == 'Text']\n",
    "            for text in text_blocks[:5]:  # Assuming the abstract appears in the first five blocks\n",
    "                if len(text.split()) > 50:\n",
    "                    abstract_text = text\n",
    "                    break\n",
    "\n",
    "        # 保存摘要块信息到CSV文件\n",
    "        df_abstract_info = pd.DataFrame(abstract_block_info)\n",
    "        df_abstract_info.to_csv(os.path.join(output_folder, 'abstract_info.csv'), index=False)\n",
    "\n",
    "        return abstract_text.strip(), first_title\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error extracting abstract: {e}\")\n",
    "        return \"\", None\n",
    "\n",
    "# 生成标题信息表格\n",
    "def generate_title_info_table(sorted_blocks, font_info_dict, output_folder):\n",
    "    title_data = []\n",
    "    for block in sorted_blocks:\n",
    "        if block.type == 'Title':\n",
    "            page_number = block.page_number\n",
    "            text = block.text.strip()\n",
    "            font_info = next((info for info in font_info_dict[page_number] if info['text'] in text), None)\n",
    "            title_data.append({\n",
    "                \"Text\": text,\n",
    "                \"Page Number\": page_number,\n",
    "                \"Block ID\": block.id,\n",
    "                \"Bold\": font_info.get(\"bold\", False) if font_info else False,\n",
    "                \"Italic\": font_info.get(\"italic\", False) if font_info else False,\n",
    "                \"Size\": font_info.get(\"size\", 0) if font_info else 0\n",
    "            })\n",
    "\n",
    "    df = pd.DataFrame(title_data)\n",
    "    df = df.sort_values(by=[\"Page Number\", \"Block ID\"]).reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "def filter_useless_title_blocks(df, title_keywords, output_folder):\n",
    "    df_filtered = df[~df['Text'].apply(lambda x: any(fuzz.partial_ratio(x.strip().lower(), keyword) > 90 for keyword in title_keywords))]\n",
    "    df_filtered.to_csv(os.path.join(output_folder, 'useful_title_block.csv'), index=False)\n",
    "    return df_filtered\n",
    "\n",
    "# 查找参考标题\n",
    "def find_reference_title(df):\n",
    "    reference_titles = [\"introduction\", \"background\", \"conclusion\",\"conclusions\", \"references\"]\n",
    "    for title in reference_titles:\n",
    "        for index, row in df.iterrows():\n",
    "            if title in row['Text'].lower():\n",
    "                logging.info(f\"Reference title found: {row['Text']}\")\n",
    "                return row\n",
    "    return None\n",
    "\n",
    "# 更新标题信息表格并标记相似标题\n",
    "def mark_similar_titles(df, reference_title, paper_title):\n",
    "    try:\n",
    "        def is_similar_row(row1, row2):\n",
    "            return (\n",
    "                row1['Bold'] == row2['Bold'] and\n",
    "                row1['Italic'] == row2['Italic'] and\n",
    "                abs(row1['Size'] - row2['Size']) <= 1\n",
    "            )\n",
    "\n",
    "        if reference_title is not None:\n",
    "            df['Similar'] = df.apply(lambda x: \"title\" if x['Text'] == paper_title else (\"Ref\" if x.equals(reference_title) else is_similar_row(x, reference_title)), axis=1)\n",
    "        else:\n",
    "            df['Similar'] = df.apply(lambda x: \"title\" if x['Text'] == paper_title else False, axis=1)\n",
    "\n",
    "        for index, row in df.iterrows():\n",
    "            if row['Similar'] != False:\n",
    "                logging.info(f\"Row {index}: {row['Text']} | Bold: {row['Bold']} | Italic: {row['Italic']} | Size: {row['Size']} | Similar: {row['Similar']}\")\n",
    "\n",
    "        for index, row in df.iterrows():\n",
    "            if row['Similar'] != False:\n",
    "                logging.info(f\"Section heading found: {row['Text']}\")\n",
    "\n",
    "        df['New ID'] = df.index + 1\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error marking similar titles: {e}\")\n",
    "        return df\n",
    "\n",
    "# 提取图像中的图文\n",
    "def extract_text_from_images(image_folder, output_folder, model, pdf_path):\n",
    "    try:\n",
    "        ocr_agent = lp.TesseractAgent(languages='eng')\n",
    "        all_text_blocks = []\n",
    "        all_captions = []\n",
    "        font_info_dict = {}\n",
    "        abstract_text = None\n",
    "        first_title = None\n",
    "        first_page_processed = False\n",
    "\n",
    "        # 提取摘要和标题\n",
    "        if not first_page_processed:\n",
    "            first_image_path = os.path.join(image_folder, 'output_page_1.png')\n",
    "            second_image_path = os.path.join(image_folder, 'output_page_2.png')\n",
    "            images = [cv2.imread(first_image_path)[..., ::-1], cv2.imread(second_image_path)[..., ::-1]]\n",
    "            abstract_text, first_title = extract_abstract_from_first_pages(images, model, output_folder)\n",
    "            first_page_processed = True\n",
    "\n",
    "        # 加载PDF一次\n",
    "        document = fitz.open(pdf_path)\n",
    "        import re\n",
    "\n",
    "        def natural_sort_key(s, _nsre=re.compile('([0-9]+)')):\n",
    "            return [int(text) if text.isdigit() else text.lower() for text in re.split(_nsre, s)]\n",
    "\n",
    "        sorted_image_files = sorted(os.listdir(image_folder), key=natural_sort_key)\n",
    "\n",
    "        def process_page(page_number, image_name):\n",
    "            try:\n",
    "                image_path = os.path.join(image_folder, image_name)\n",
    "                image = cv2.imread(image_path)\n",
    "                image = image[..., ::-1]\n",
    "\n",
    "                page = document.load_page(page_number - 1)\n",
    "                font_info = extract_combined_font_info(page, image_path)\n",
    "                if not font_info:\n",
    "                    logging.warning(f\"Font info extraction failed for page {page_number}\")\n",
    "\n",
    "                layout = model.detect(image)\n",
    "                if layout is None:\n",
    "                    raise ValueError(\"Model detection returned None\")\n",
    "\n",
    "                for block in layout:\n",
    "                    segment_image = block.pad(left=5, right=5, top=5, bottom=5).crop_image(image)\n",
    "                    text = ocr_agent.detect(segment_image)\n",
    "                    block.set(text=text, inplace=True)\n",
    "                    block.page_number = page_number\n",
    "\n",
    "                text_blocks = lp.Layout([b for b in layout if b.type in ['Text', 'Title', 'List']])\n",
    "                figure_blocks = lp.Layout([b for b in layout if b.type in ['Figure', 'Table']])\n",
    "                text_blocks = lp.Layout([b for b in text_blocks if not any(b.is_in(b_fig) for b_fig in figure_blocks)])\n",
    "                \n",
    "                sorted_blocks = sort_blocks_by_coordinates(text_blocks)\n",
    "\n",
    "                if sorted_blocks:  # 检查是否为空\n",
    "                    for idx, block in enumerate(sorted_blocks):\n",
    "                        block.id = idx  # 直接设置id属性\n",
    "\n",
    "                captions = []\n",
    "                filtered_text_blocks = []\n",
    "\n",
    "                for block in sorted_blocks:\n",
    "                    if is_potential_caption(block, figure_blocks):\n",
    "                        captions.append(block)\n",
    "                    else:\n",
    "                        filtered_text_blocks.append(block)\n",
    "\n",
    "                image_with_boxes = draw_box(image, sorted_blocks)\n",
    "                output_image_filename = os.path.join(output_folder, 'images_with_boxes', f\"{os.path.splitext(image_name)[0]}_with_boxes.png\")\n",
    "                cv2.imwrite(output_image_filename, image_with_boxes)\n",
    "\n",
    "                return filtered_text_blocks, captions, font_info\n",
    "\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error processing page {page_number}: {e}\")\n",
    "                return [], [], []\n",
    "\n",
    "        # 使用多线程并行处理每一页\n",
    "        with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "            futures = [executor.submit(process_page, int(re.search(r'\\d+', image_name).group()), image_name) for image_name in sorted_image_files]\n",
    "            for future in as_completed(futures):\n",
    "                filtered_text_blocks, captions, font_info = future.result()\n",
    "                all_text_blocks.extend(filtered_text_blocks)\n",
    "                all_captions.extend(captions)\n",
    "                if font_info:\n",
    "                    page_number = filtered_text_blocks[0].page_number if filtered_text_blocks else captions[0].page_number\n",
    "                    font_info_dict[page_number] = font_info\n",
    "\n",
    "        os.makedirs(os.path.join(output_folder, 'text'), exist_ok=True)\n",
    "        os.makedirs(os.path.join(output_folder, 'pickle'), exist_ok=True)\n",
    "\n",
    "        with open(os.path.join(output_folder, 'text', 'captions.txt'), 'w', encoding='utf-8') as caption_file:\n",
    "            for caption in all_captions:\n",
    "                caption_file.write(caption.text + \"\\n\")\n",
    "        \n",
    "        all_text_blocks.sort(key=lambda b: (b.page_number, b.id))\n",
    "\n",
    "        if abstract_text:\n",
    "            with open(os.path.join(output_folder, 'text', 'Abstract.txt'), 'w', encoding='utf-8') as abstract_file:\n",
    "                abstract_file.write(abstract_text)\n",
    "            with open(os.path.join(output_folder, 'pickle', 'Abstract.pkl'), 'wb') as abstract_pkl_file:\n",
    "                pickle.dump(abstract_text, abstract_pkl_file)\n",
    "\n",
    "        return all_text_blocks, font_info_dict, abstract_text, first_title\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error extracting text from images: {e}\")\n",
    "        return [], {}, \"\", None\n",
    "\n",
    "# 提取段落内容并保存\n",
    "def extract_and_save_sections(sorted_blocks, df, output_folder):\n",
    "    sections = {}\n",
    "    current_section = None\n",
    "\n",
    "    for block in sorted_blocks:\n",
    "        if block.type == 'Title':\n",
    "            text = block.text.strip()\n",
    "            if re.match(r'^\\d+(\\.\\d+)?', text):\n",
    "                if re.match(r'^\\d+\\.\\d+', text):\n",
    "                    if current_section:\n",
    "                        sections[current_section].append(text)\n",
    "                else:\n",
    "                    current_section = text\n",
    "                    sections[current_section] = []\n",
    "            elif text.lower() in df[df['Similar'].isin([True, 'Ref'])]['Text'].str.lower().tolist():\n",
    "                current_section = text\n",
    "                sections[current_section] = []\n",
    "\n",
    "        if current_section and block.type != 'Title':\n",
    "            sections[current_section].append(block.text.strip())\n",
    "\n",
    "    section_order = []\n",
    "    for section, texts in sections.items():\n",
    "        section_text = \" \".join(texts).replace('\\n', ' ')\n",
    "        filename = f\"{section}.pkl\".replace(\" \", \"_\").replace(\"/\", \"_\")\n",
    "        section_order.append((section, filename))\n",
    "        with open(os.path.join(output_folder, 'pickle', filename), 'wb') as f:\n",
    "            pickle.dump(section_text, f)\n",
    "        with open(os.path.join(output_folder, 'text', f\"{section}.txt\".replace(\" \", \"_\").replace(\"/\", \"_\")), 'w', encoding='utf-8') as section_file:\n",
    "            section_file.write(section_text)\n",
    "        logging.info(f\"Saved {section} to {filename}\")\n",
    "\n",
    "    return section_order\n",
    "\n",
    "# 生成CSV文件\n",
    "def generate_csv(output_root_folder, df_data):\n",
    "    rows = []\n",
    "    for data in df_data:\n",
    "        row = {'PDF name': data['pdf_name'], 'article': data['paper_title'], 'abstract': data['abstract']}\n",
    "        for idx, (section, filename) in enumerate(data['section_order'], start=1):\n",
    "            if filename == 'Abstract.pkl':\n",
    "                continue\n",
    "            row[f'section {idx}'] = filename\n",
    "        rows.append(row)\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    df.to_csv(os.path.join(output_root_folder, 'all_papers.csv'), index=False)\n",
    "\n",
    "# 处理PDF文件夹中的所有PDF\n",
    "def process_pdfs_in_folder(pdf_folder, output_root_folder, model):\n",
    "    try:\n",
    "        df_data = []\n",
    "\n",
    "        for pdf_file in os.listdir(pdf_folder):\n",
    "            if pdf_file.endswith('.pdf'):\n",
    "                pdf_path = os.path.join(pdf_folder, pdf_file)\n",
    "                pdf_name = os.path.splitext(pdf_file)[0]\n",
    "                pdf_output_folder = os.path.join(output_root_folder, pdf_name)\n",
    "\n",
    "                for folder in ['images', 'images_with_boxes', 'text', 'pickle']:\n",
    "                    os.makedirs(os.path.join(pdf_output_folder, folder), exist_ok=True)\n",
    "\n",
    "                render_pdf_pages_to_images(pdf_path, os.path.join(pdf_output_folder, 'images'))\n",
    "\n",
    "                all_text_blocks, font_info_dict, abstract_text, first_title = extract_text_from_images(os.path.join(pdf_output_folder, 'images'), pdf_output_folder, model, pdf_path)\n",
    "                \n",
    "                df = generate_title_info_table(all_text_blocks, font_info_dict, pdf_output_folder)\n",
    "\n",
    "                reference_title = find_reference_title(df)\n",
    "\n",
    "                df = mark_similar_titles(df, reference_title, first_title)\n",
    "                df.to_csv(os.path.join(pdf_output_folder, 'title_blocks_info.csv'), index=False)\n",
    "\n",
    "                section_order = extract_and_save_sections(all_text_blocks, df, pdf_output_folder)\n",
    "                \n",
    "                # 排序 section_order 按照 df 中的 New ID 顺序\n",
    "                sorted_section_order = sorted(section_order, key=lambda x: df[df['Text'] == x[0]]['New ID'].values[0] if x[0] in df['Text'].values else float('inf'))\n",
    "\n",
    "                df_data.append({\n",
    "                    'pdf_name': pdf_name,\n",
    "                    'paper_title': first_title,\n",
    "                    'abstract': 'Abstract.pkl',\n",
    "                    'section_order': sorted_section_order\n",
    "                })\n",
    "\n",
    "        generate_csv(output_root_folder, df_data)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing PDFs in folder: {e}\")\n",
    "\n",
    "# 使用示例\n",
    "pdf_folder = 'Diabetes PDFs'\n",
    "output_root_folder = 'Diabetes PDFs Outputs'\n",
    "\n",
    "process_pdfs_in_folder(pdf_folder, output_root_folder, model)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. create a detectron2 environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Scraping PDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/bin:/Users/billionaire/opt/anaconda3/envs/detectron2/bin:/Users/billionaire/opt/anaconda3/condabin:/opt/homebrew/bin:/opt/homebrew/sbin:/usr/local/bin:/System/Cryptexes/App/usr/bin:/usr/bin:/bin:/usr/sbin:/sbin:/var/run/com.apple.security.cryptexd/codex.system/bootstrap/usr/local/bin:/var/run/com.apple.security.cryptexd/codex.system/bootstrap/usr/bin:/var/run/com.apple.security.cryptexd/codex.system/bootstrap/usr/appleinternal/bin:/opt/X11/bin\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['PATH'] = \"/opt/homebrew/bin:\" + os.environ['PATH']\n",
    "!echo $PATH\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tesseract 5.4.1\n",
      " leptonica-1.84.1\n",
      "  libgif 5.2.1 : libjpeg 8d (libjpeg-turbo 3.0.0) : libpng 1.6.43 : libtiff 4.6.0 : zlib 1.2.12 : libwebp 1.4.0 : libopenjp2 2.5.2\n",
      " Found NEON\n",
      " Found libarchive 3.7.4 zlib/1.2.12 liblzma/5.4.6 bz2lib/1.0.8 liblz4/1.9.4 libzstd/1.5.6\n",
      " Found libcurl/8.1.2 SecureTransport (LibreSSL/3.3.6) zlib/1.2.12 nghttp2/1.55.1\n"
     ]
    }
   ],
   "source": [
    "!/opt/homebrew/bin/tesseract -v\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/billionaire/opt/anaconda3/envs/detectron2/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "sysctl: unknown oid 'machdep.cpu.leaf7_features'\n",
      "WARNING: AVX is not support on your machine. Hence, no_avx core will be imported, It has much worse preformance than avx core.\n"
     ]
    }
   ],
   "source": [
    "import layoutparser as lp\n",
    "import cv2\n",
    "\n",
    "# Load the model with the local path\n",
    "model = lp.Detectron2LayoutModel(\n",
    "    config_path='model/config.yaml',\n",
    "    model_path='model/model_final.pth',\n",
    "    label_map={0: \"Text\", 1: \"Title\", 2: \"List\", 3: \"Table\", 4: \"Figure\"},\n",
    "    extra_config=[\"MODEL.ROI_HEADS.SCORE_THRESH_TEST\", 0.8]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF\n",
    "import os\n",
    "import layoutparser as lp\n",
    "import cv2\n",
    "import pytesseract\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import re\n",
    "import numpy as np\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "# Convert PDFs to images\n",
    "def render_pdf_pages_to_images(pdf_path, image_folder, zoom=2):\n",
    "    if not os.path.exists(image_folder):\n",
    "        os.makedirs(image_folder)\n",
    "\n",
    "    document = fitz.open(pdf_path)\n",
    "    for page_number, page in enumerate(document):\n",
    "        mat = fitz.Matrix(zoom, zoom)\n",
    "        pix = page.get_pixmap(matrix=mat)\n",
    "        image_filename = f\"{image_folder}/output_page_{page_number + 1}.png\"\n",
    "        pix.save(image_filename)\n",
    "        print(f\"Saved {image_filename}\")\n",
    "\n",
    "    document.close()\n",
    "\n",
    "# Extract font information\n",
    "def extract_font_info(page):\n",
    "    font_info = []\n",
    "    blocks = page.get_text(\"dict\")[\"blocks\"]\n",
    "    for block in blocks:\n",
    "        if \"lines\" in block:\n",
    "            for line in block[\"lines\"]:\n",
    "                for span in line[\"spans\"]:\n",
    "                    font_info.append({\n",
    "                        \"bold\": span[\"flags\"] & 2 != 0,\n",
    "                        \"italic\": span[\"flags\"] & 1 != 0,\n",
    "                        \"size\": span[\"size\"],\n",
    "                        \"text\": span[\"text\"]\n",
    "                    })\n",
    "    return font_info\n",
    "\n",
    "# Check captions\n",
    "\n",
    "def is_potential_caption(block, figure_blocks, distance=20, word_limit=50):\n",
    "    text = block.text\n",
    "    if \"Â©\" in text:\n",
    "        return True\n",
    "\n",
    "    # Check useless title\n",
    "    title_keywords = [\"funding\", \"conflict of interest\", \"supplementary materials\", \"declaration\",\n",
    "                      \"acknowledgments\", \"Data availability\", \"Author contributions\", \"Publisher's note\", \"appendix\"]\n",
    "\n",
    "\n",
    "    if block.type == 'Title':\n",
    "        for keyword in title_keywords:\n",
    "            if fuzz.partial_ratio(text.lower(), keyword) > 50:  \n",
    "                return False\n",
    "\n",
    "    if re.match(r'^(Fig\\.|Figure|Table|Box)\\s*\\d+|^(supplementary\\w*|appendix\\w*|fund\\w*|conflict of interest\\w*|Data availability\\w*|Publi\\w*|Abbreviation\\w*|Author\\w*|The Author|Copyright|Correspond\\w*|Assess\\w*|Email\\w*|Tel\\w*|Open access|Keywords|Address\\w*|Receive\\w*|Review\\w*)', text, re.IGNORECASE):\n",
    "        return True\n",
    "\n",
    "    for figure_block in figure_blocks:\n",
    "        x0, y0, x1, y1 = figure_block.coordinates\n",
    "        bx0, by0, bx1, by1 = block.coordinates\n",
    "\n",
    "        # Check if the block is within 20 pixels of figure_block\n",
    "        if (\n",
    "            abs(y0 - by1) <= distance or abs(y1 - by0) <= distance or\n",
    "            abs(x0 - bx1) <= distance or abs(x1 - bx0) <= distance\n",
    "        ):\n",
    "            # Check if the text block has less than 50 characters\n",
    "            if len(block.text.strip().split()) < word_limit:\n",
    "                return True\n",
    "    return False \n",
    "\n",
    "\n",
    "\n",
    "TOLERANCE = 50\n",
    "\n",
    "# Sort by x coordinate first, then by y coordinate within a certain tolerance\n",
    "def sort_blocks_by_coordinates(blocks, tolerance=TOLERANCE):\n",
    "    if not blocks:  \n",
    "        return []\n",
    "\n",
    "    blocks = sorted(blocks, key=lambda b: b.coordinates[0])\n",
    "    \n",
    "    sorted_blocks = []\n",
    "    current_line = []\n",
    "    current_x = blocks[0].coordinates[0]\n",
    "    \n",
    "    for block in blocks:\n",
    "        if abs(block.coordinates[0] - current_x) <= tolerance:\n",
    "            current_line.append(block)\n",
    "        else:\n",
    "            current_line.sort(key=lambda b: b.coordinates[1])\n",
    "            sorted_blocks.extend(current_line)\n",
    "            current_line = [block]\n",
    "            current_x = block.coordinates[0]\n",
    "    \n",
    "\n",
    "    current_line.sort(key=lambda b: b.coordinates[1])\n",
    "    sorted_blocks.extend(current_line)\n",
    "    \n",
    "    return sorted_blocks\n",
    "\n",
    "def draw_box(image, layout, box_width=3, show_element_id=True):\n",
    "\n",
    "    pil_image = Image.fromarray(image)\n",
    "\n",
    "    draw = ImageDraw.Draw(pil_image)\n",
    "\n",
    "    for element in layout:\n",
    "        box = element.coordinates\n",
    "        draw.rectangle(box, outline=\"red\", width=box_width)\n",
    "\n",
    "        if show_element_id and hasattr(element, 'id'):\n",
    "            font = ImageFont.load_default()\n",
    "            text = f\"ID: {element.id}\"\n",
    "            text_bbox = font.getbbox(text)\n",
    "            text_width = text_bbox[2] - text_bbox[0]\n",
    "            text_height = text_bbox[3] - text_bbox[1]\n",
    "            draw.rectangle([box[0], box[1] - text_height, box[0] + text_width, box[1]], fill=\"red\")\n",
    "            draw.text((box[0], box[1] - text_height), text, fill=\"white\", font=font)\n",
    "\n",
    "    image = cv2.cvtColor(np.array(pil_image), cv2.COLOR_RGB2BGR)\n",
    "    return image\n",
    "\n",
    "# Extract abstract and title\n",
    "\n",
    "def extract_abstract_from_first_pages(images, model):\n",
    "    ocr_agent = lp.TesseractAgent(languages='eng')\n",
    "    abstract_text = \"\"\n",
    "    first_title = None\n",
    "\n",
    "    for image in images:\n",
    "        layout = model.detect(image)\n",
    "\n",
    "        for idx, block in enumerate(layout):\n",
    "            block.id = idx\n",
    "            segment_image = block.pad(left=5, right=5, top=5, bottom=5).crop_image(image)\n",
    "            text = ocr_agent.detect(segment_image).strip()\n",
    "            block.set(text=text, inplace=True)\n",
    "        \n",
    "        abstract_block = None\n",
    "\n",
    "        # Step 1: Look for an \"Abstract\" title block\n",
    "        for block in layout:\n",
    "            if block.type == 'Title' and block.text.strip().lower() == \"abstract\":\n",
    "                abstract_block = block\n",
    "                break\n",
    "\n",
    "        if abstract_block and abstract_block.id is not None:\n",
    "            # Collect text blocks below the \"Abstract\" title block\n",
    "            for block in layout:\n",
    "                if block.id is not None and block.id > abstract_block.id:\n",
    "                    if block.type == 'Title' and block.text.strip().lower() == \"introduction\":\n",
    "                        break  # Stop at the \"Introduction\" title block\n",
    "                    abstract_text += block.text.strip() + \" \"\n",
    "                    if len(block.text.strip()) == 0:\n",
    "                        break\n",
    "\n",
    "        if not abstract_text.strip():\n",
    "            # Step 2: Keyword matching\n",
    "            keywords = [\"aim\", \"aims\", \"background\", \"purpose\", \"purposes\", \"introduction\", \"objective\", \"objectives\", \n",
    "                        \"method\", \"methods\", \"material\",\"materials\", \"materials and methods\", \n",
    "                        \"result\",\"results\", \"conclusion\", \"conclusions\", \"discussion\"]\n",
    "            keyword_blocks = []\n",
    "            \n",
    "            for block in layout:\n",
    "                text = block.text.strip().lower()\n",
    "                if any(keyword in text for keyword in keywords):\n",
    "                    keyword_blocks.append(block)\n",
    "            \n",
    "            if keyword_blocks:\n",
    "                # Sort keyword blocks by y-coordinate and collect nearby blocks within 20 pixels\n",
    "                keyword_blocks = sorted(keyword_blocks, key=lambda b: b.coordinates[1])\n",
    "                for keyword_block in keyword_blocks:\n",
    "                    for block in layout:\n",
    "                        if abs(block.coordinates[1] - keyword_block.coordinates[1]) <= 20:\n",
    "                            abstract_text += block.text.strip() + \" \"\n",
    "\n",
    "        if not first_title:\n",
    "            # Look for the first title that is not \"Abstract\"\n",
    "            for block in layout:\n",
    "                if block.type == 'Title' and block.text.strip().lower() != \"abstract\":\n",
    "                    first_title = block.text.strip().replace('\\n', ' ')\n",
    "                    break\n",
    "\n",
    "    if not abstract_text.strip():\n",
    "        # Step 3: Heuristic method\n",
    "        text_blocks = [block.text.strip() for page_layout in layout for block in page_layout if block.type == 'Text']\n",
    "        for text in text_blocks[:5]:  # Assuming the abstract appears in the first five blocks\n",
    "            if len(text.split()) > 50:\n",
    "                abstract_text = text\n",
    "                break\n",
    "\n",
    "    return abstract_text.strip(), first_title\n",
    "\n",
    "\n",
    "# Generate title information tables\n",
    "def generate_title_info_table(sorted_blocks, font_info_dict, output_folder):\n",
    "    title_data = []\n",
    "    for block in sorted_blocks:\n",
    "        if block.type == 'Title':\n",
    "            page_number = block.page_number\n",
    "            text = block.text.strip()\n",
    "            font_info = next((info for info in font_info_dict[page_number] if info['text'] in text), None)\n",
    "            title_data.append({\n",
    "                \"Text\": text,\n",
    "                \"Bold\": font_info.get(\"bold\", False) if font_info else False,\n",
    "                \"Italic\": font_info.get(\"italic\", False) if font_info else False,\n",
    "                \"Size\": font_info.get(\"size\", 0) if font_info else 0\n",
    "            })\n",
    "\n",
    "    df = pd.DataFrame(title_data)\n",
    "    df.to_csv(os.path.join(output_folder, 'title_blocks_info.csv'), index=False)\n",
    "    return df\n",
    "\n",
    "# Find reference titles\n",
    "def find_reference_title(df):\n",
    "    reference_titles = [\"introduction\", \"background\", \"conclusion\",\"conclusions\", \"references\"]\n",
    "    for title in reference_titles:\n",
    "        for index, row in df.iterrows():\n",
    "            if title in row['Text'].lower():\n",
    "                print(f\"Reference title found: {row['Text']}\")\n",
    "                return row\n",
    "    return None\n",
    "\n",
    "\n",
    "# Label similar section headings\n",
    "def mark_similar_titles(df, reference_title, paper_title):\n",
    "    def is_similar_row(row1, row2):\n",
    "        return (\n",
    "            row1['Bold'] == row2['Bold'] and\n",
    "            row1['Italic'] == row2['Italic'] and\n",
    "            abs(row1['Size'] - row2['Size']) <= 1\n",
    "        )\n",
    "\n",
    "    if reference_title is not None:\n",
    "        df['Similar'] = df.apply(lambda x: \"title\" if x['Text'] == paper_title else (\"Ref\" if x.equals(reference_title) else is_similar_row(x, reference_title)), axis=1)\n",
    "    else:\n",
    "        df['Similar'] = df.apply(lambda x: \"title\" if x['Text'] == paper_title else False, axis=1)\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        if row['Similar'] != False:\n",
    "            print(f\"Row {index}: {row['Text']} | Bold: {row['Bold']} | Italic: {row['Italic']} | Size: {row['Size']} | Similar: {row['Similar']}\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# Extract text from images\n",
    "\n",
    "def extract_text_from_images(image_folder, output_folder, model, pdf_path):\n",
    "    ocr_agent = lp.TesseractAgent(languages='eng')\n",
    "    all_text_blocks = []\n",
    "    all_captions = []\n",
    "    font_info_dict = {}\n",
    "    abstract_text = None\n",
    "    first_title = None\n",
    "    document = fitz.open(pdf_path)\n",
    "    first_page_processed = False\n",
    "\n",
    "    first_image_path = os.path.join(image_folder, 'output_page_1.png')\n",
    "    second_image_path = os.path.join(image_folder, 'output_page_2.png')\n",
    "    images = [cv2.imread(first_image_path)[..., ::-1], cv2.imread(second_image_path)[..., ::-1]]\n",
    "\n",
    "    # Extract abstract and title\n",
    "    if not first_page_processed:\n",
    "        abstract_text, first_title = extract_abstract_from_first_pages(images, model)\n",
    "        first_page_processed = True\n",
    "\n",
    "    for image_name in sorted(os.listdir(image_folder)):\n",
    "        if image_name.endswith('.png'):\n",
    "            page_number = int(re.search(r'\\d+', image_name).group())\n",
    "            image_path = os.path.join(image_folder, image_name)\n",
    "            image = cv2.imread(image_path)\n",
    "            image = image[..., ::-1]\n",
    "\n",
    "            page = document.load_page(page_number - 1)\n",
    "            font_info_dict[page_number] = extract_font_info(page)\n",
    "\n",
    "            layout = model.detect(image)\n",
    "\n",
    "            for block in layout:\n",
    "                segment_image = block.pad(left=5, right=5, top=5, bottom=5).crop_image(image)\n",
    "                text = ocr_agent.detect(segment_image)\n",
    "                block.set(text=text, inplace=True)\n",
    "                block.page_number = page_number\n",
    "\n",
    "            text_blocks = lp.Layout([b for b in layout if b.type in ['Text', 'Title', 'List']])\n",
    "            figure_blocks = lp.Layout([b for b in layout if b.type in ['Figure', 'Table']])\n",
    "            text_blocks = lp.Layout([b for b in text_blocks if not any(b.is_in(b_fig) for b_fig in figure_blocks)])\n",
    "            \n",
    "            sorted_blocks = sort_blocks_by_coordinates(text_blocks)\n",
    "\n",
    "            if sorted_blocks:  \n",
    "                for idx, block in enumerate(sorted_blocks):\n",
    "                    block.id = idx  # set id \n",
    "\n",
    "            captions = []\n",
    "            filtered_text_blocks = []\n",
    "\n",
    "            for block in sorted_blocks:\n",
    "                if is_potential_caption(block, figure_blocks):\n",
    "                    captions.append(block)\n",
    "                else:\n",
    "                    filtered_text_blocks.append(block)\n",
    "\n",
    "            all_captions.extend(captions)\n",
    "            all_text_blocks.extend(filtered_text_blocks)\n",
    "\n",
    "            image_with_boxes = draw_box(image, sorted_blocks)\n",
    "            output_image_filename = os.path.join(output_folder, 'images_with_boxes', f\"{os.path.splitext(image_name)[0]}_with_boxes.png\")\n",
    "            cv2.imwrite(output_image_filename, image_with_boxes)\n",
    "\n",
    "    os.makedirs(os.path.join(output_folder, 'text'), exist_ok=True)\n",
    "    os.makedirs(os.path.join(output_folder, 'pickle'), exist_ok=True)\n",
    "\n",
    "    with open(os.path.join(output_folder, 'text', 'captions.txt'), 'w', encoding='utf-8') as caption_file:\n",
    "        for caption in all_captions:\n",
    "            caption_file.write(caption.text + \"\\n\")\n",
    "    \n",
    "    all_text_blocks.sort(key=lambda b: (b.page_number, b.id))\n",
    "\n",
    "    \n",
    "    if abstract_text:\n",
    "        with open(os.path.join(output_folder, 'text', 'Abstract.txt'), 'w', encoding='utf-8') as abstract_file:\n",
    "            abstract_file.write(abstract_text)\n",
    "        with open(os.path.join(output_folder, 'pickle', 'Abstract.pkl'), 'wb') as abstract_pkl_file:\n",
    "            pickle.dump(abstract_text, abstract_pkl_file)\n",
    "\n",
    "    return all_text_blocks, font_info_dict, abstract_text, first_title\n",
    "\n",
    "\n",
    "\n",
    "# Extract text from images\n",
    "def extract_and_save_sections(sorted_blocks, df, output_folder):\n",
    "    sections = {}\n",
    "    current_section = None\n",
    "\n",
    "    for block in sorted_blocks:\n",
    "        # Step 1: Check if block is a numbered title\n",
    "        if block.type == 'Title':\n",
    "            text = block.text.strip()\n",
    "            if re.match(r'^\\d+(\\.\\d+)?', text):  # Check if the title starts with a number or a decimal\n",
    "                if re.match(r'^\\d+\\.\\d+', text):  # Skip subheadings like \"1.1\", \"2.1\"\n",
    "                    if current_section:\n",
    "                        sections[current_section].append(text)\n",
    "                else:\n",
    "                    current_section = text\n",
    "                    sections[current_section] = []\n",
    "            elif text.lower() in df[df['Similar'].isin([True, 'Ref'])]['Text'].str.lower().tolist():\n",
    "                current_section = text\n",
    "                sections[current_section] = []\n",
    "\n",
    "        # Step 2: Collect text under the current section\n",
    "        if current_section:\n",
    "            sections[current_section].append(block.text.strip())\n",
    "\n",
    "    section_order = []\n",
    "    for section, texts in sections.items():\n",
    "        section_text = \" \".join(texts).replace('\\n', ' ')\n",
    "        filename = f\"{section}.pkl\".replace(\" \", \"_\").replace(\"/\", \"_\")\n",
    "        section_order.append((section, filename))\n",
    "        with open(os.path.join(output_folder, 'pickle', filename), 'wb') as f:\n",
    "            pickle.dump(section_text, f)\n",
    "        with open(os.path.join(output_folder, 'text', f\"{section}.txt\".replace(\" \", \"_\").replace(\"/\", \"_\")), 'w', encoding='utf-8') as section_file:\n",
    "            section_file.write(section_text)\n",
    "        print(f\"Saved {section} to {filename}\")\n",
    "\n",
    "    return section_order\n",
    "\n",
    "# Generate csv\n",
    "def generate_csv(output_root_folder, df_data):\n",
    "    rows = []\n",
    "    for data in df_data:\n",
    "        row = {'PDF name': data['pdf_name'], 'article': data['paper_title'], 'abstract': data['abstract']}\n",
    "        for idx, (section, filename) in enumerate(data['section_order'], start=1):\n",
    "            if filename == 'Abstract.pkl':\n",
    "                continue\n",
    "            row[f'section {idx}'] = filename\n",
    "        rows.append(row)\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    df.to_csv(os.path.join(output_root_folder, 'all_papers.csv'), index=False)\n",
    "\n",
    "# process the whole folder\n",
    "\n",
    "def process_pdfs_in_folder(pdf_folder, output_root_folder, model):\n",
    "    df_data = []\n",
    "\n",
    "    for pdf_file in os.listdir(pdf_folder):\n",
    "        if pdf_file.endswith('.pdf'):\n",
    "            pdf_path = os.path.join(pdf_folder, pdf_file)\n",
    "            pdf_name = os.path.splitext(pdf_file)[0]\n",
    "            pdf_output_folder = os.path.join(output_root_folder, pdf_name)\n",
    "\n",
    "            # create folders\n",
    "            for folder in ['images', 'images_with_boxes', 'text', 'pickle']:\n",
    "                os.makedirs(os.path.join(pdf_output_folder, folder), exist_ok=True)\n",
    "\n",
    "            # Convert PDFs into images\n",
    "            render_pdf_pages_to_images(pdf_path, os.path.join(pdf_output_folder, 'images'))\n",
    "\n",
    "            # Extract text and save them\n",
    "            all_text_blocks, font_info_dict, abstract_text, first_title = extract_text_from_images(os.path.join(pdf_output_folder, 'images'), pdf_output_folder, model, pdf_path)\n",
    "\n",
    "            # Find reference titles\n",
    "            df = generate_title_info_table(all_text_blocks, font_info_dict, pdf_output_folder)\n",
    "            reference_title = find_reference_title(df)\n",
    "\n",
    "            # Label the similar headings\n",
    "            df = mark_similar_titles(df, reference_title, first_title)\n",
    "            df.to_csv(os.path.join(pdf_output_folder, 'title_blocks_info.csv'), index=False)\n",
    "\n",
    "            # Save sections\n",
    "            section_order = extract_and_save_sections(all_text_blocks, df, pdf_output_folder)\n",
    "            \n",
    "            df_data.append({\n",
    "                'pdf_name': pdf_name,\n",
    "                'paper_title': first_title,\n",
    "                'abstract': 'Abstract.pkl',\n",
    "                'section_order': section_order\n",
    "            })\n",
    "\n",
    "    generate_csv(output_root_folder, df_data)\n",
    "\n",
    "\n",
    "\n",
    "pdf_folder = 'Diabetes PDFs'\n",
    "output_root_folder = 'Diabetes PDFs Outputs'\n",
    "\n",
    "process_pdfs_in_folder(pdf_folder, output_root_folder, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Combine sections "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PDF name</th>\n",
       "      <th>article</th>\n",
       "      <th>abstract</th>\n",
       "      <th>section 1</th>\n",
       "      <th>section 2</th>\n",
       "      <th>section 3</th>\n",
       "      <th>section 4</th>\n",
       "      <th>section 5</th>\n",
       "      <th>section 6</th>\n",
       "      <th>section 7</th>\n",
       "      <th>section 8</th>\n",
       "      <th>section 9</th>\n",
       "      <th>section 10</th>\n",
       "      <th>section 11</th>\n",
       "      <th>section 12</th>\n",
       "      <th>section 13</th>\n",
       "      <th>section 14</th>\n",
       "      <th>section 15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>s12889-024-18580-0</td>\n",
       "      <td>Prevalence of metabolic syndrome  and associat...</td>\n",
       "      <td>Abstract.pkl</td>\n",
       "      <td>Introduction.pkl</td>\n",
       "      <td>Methods.pkl</td>\n",
       "      <td>Result.pkl</td>\n",
       "      <td>Prevalence_of_metabolic_syndrome_among_type_2_...</td>\n",
       "      <td>Association_between_BMI_and_prevalence_of_meta...</td>\n",
       "      <td>Conclusion.pkl</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Evaluation of the Lifetime Benefits of Metform...</td>\n",
       "      <td>Evaluation of the Lifetime Benefits of Metform...</td>\n",
       "      <td>Abstract.pkl</td>\n",
       "      <td>1_Introduction.pkl</td>\n",
       "      <td>2_Methods.pkl</td>\n",
       "      <td>3_Results.pkl</td>\n",
       "      <td>4_Discussion.pkl</td>\n",
       "      <td>5_Conclusion.pkl</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>regulatory_patterns_of_chinese_patent_medicine...</td>\n",
       "      <td>Regulatory patterns of Chinese patent medicine...</td>\n",
       "      <td>Abstract.pkl</td>\n",
       "      <td>1._Background.pkl</td>\n",
       "      <td>2._Methods.pkl</td>\n",
       "      <td>3._Results.pkl</td>\n",
       "      <td>4._Conclusion.pkl</td>\n",
       "      <td>5._Conclusion.pkl</td>\n",
       "      <td>Author_contributions.pkl</td>\n",
       "      <td>References.pkl</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Association of Common Genetic Variants in Mito...</td>\n",
       "      <td>Association of Common Genetic Variants in Mito...</td>\n",
       "      <td>Abstract.pkl</td>\n",
       "      <td>INTRODUCTION.pkl</td>\n",
       "      <td>MetHops.pkl</td>\n",
       "      <td>Resutts.pkl</td>\n",
       "      <td>Discussion.pkl</td>\n",
       "      <td>REFERENCES.pkl</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>s00125-024-06144-1</td>\n",
       "      <td>Subcutaneously administered tirzepatide vs sem...</td>\n",
       "      <td>Abstract.pkl</td>\n",
       "      <td>Introduction.pkl</td>\n",
       "      <td>Methods.pkl</td>\n",
       "      <td>Results.pkl</td>\n",
       "      <td>Discussion.pkl</td>\n",
       "      <td>References.pkl</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Evidence that tirzepatide protects against dia...</td>\n",
       "      <td>Evidence that tirzepatide protects against dia...</td>\n",
       "      <td>Abstract.pkl</td>\n",
       "      <td>Background.pkl</td>\n",
       "      <td>Methods.pkl</td>\n",
       "      <td>Meta-analysis:_search_strategy,_selection_crit...</td>\n",
       "      <td>Results.pkl</td>\n",
       "      <td>Meta-analysis_of_clinical_results.pkl</td>\n",
       "      <td>Evidence_that_tirzepatide_protects\\nagainst_di...</td>\n",
       "      <td>Cardioprotective_effect_of_TZT_by_meta-analysi...</td>\n",
       "      <td>In_vitro_results_in_AC16_cell_line_results_in_...</td>\n",
       "      <td>Protective_Effects_of_TZT_on_cell_proliferatio...</td>\n",
       "      <td>Conclusions.pkl</td>\n",
       "      <td>Supplementary_Information.pkl</td>\n",
       "      <td>Publisherâs_Note.pkl</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>fendo-14-1285147</td>\n",
       "      <td>Efficacy and safety of insulin glargine 300 un...</td>\n",
       "      <td>Abstract.pkl</td>\n",
       "      <td>Background.pkl</td>\n",
       "      <td>Objectives.pkl</td>\n",
       "      <td>Methods.pkl</td>\n",
       "      <td>Results.pkl</td>\n",
       "      <td>Results_of_the_search.pkl</td>\n",
       "      <td>Discussion.pkl</td>\n",
       "      <td>Conclusion.pkl</td>\n",
       "      <td>Data_availability_statement.pkl</td>\n",
       "      <td>Author_contributions.pkl</td>\n",
       "      <td>References.pkl</td>\n",
       "      <td>Funding.pkl</td>\n",
       "      <td>Acknowledgments.pkl</td>\n",
       "      <td>Conflict_of_interest.pkl</td>\n",
       "      <td>Publisher's_note.pkl</td>\n",
       "      <td>Supplementary_material.pkl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>fcdhc-03-947552</td>\n",
       "      <td>Unpacking determinants and consequences of foo...</td>\n",
       "      <td>Abstract.pkl</td>\n",
       "      <td>Introduction.pkl</td>\n",
       "      <td>Objectives.pkl</td>\n",
       "      <td>Diving_downstream:_Food_insecurity_as_a\\npoten...</td>\n",
       "      <td>Implications_for_nutrition_equity.pkl</td>\n",
       "      <td>Overview_of_study_setting_and_design.pkl</td>\n",
       "      <td>Blood_sampling,_processing,_and_analysis.pkl</td>\n",
       "      <td>Variables.pkl</td>\n",
       "      <td>Exit_interview_and_participant_follow-up_calls...</td>\n",
       "      <td>Variable.pkl</td>\n",
       "      <td>Data_storage_and_management.pkl</td>\n",
       "      <td>Statistical_analysis.pkl</td>\n",
       "      <td>Insulin_sensitivity.pkl</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Visceral adiposity index as a predictor of typ...</td>\n",
       "      <td>Visceral adiposity index as a predictor of typ...</td>\n",
       "      <td>Abstract.pkl</td>\n",
       "      <td>1._Introduction.pkl</td>\n",
       "      <td>2._Materials_and_methods.pkl</td>\n",
       "      <td>3._Results.pkl</td>\n",
       "      <td>4._Discussion.pkl</td>\n",
       "      <td>5._Conclusion.pkl</td>\n",
       "      <td>Authorsâ_contributions.pkl</td>\n",
       "      <td>Funding.pkl</td>\n",
       "      <td>Disclosure_of_potential_conflicts_of_interest.pkl</td>\n",
       "      <td>Appendix_A._Supplementary_data.pkl</td>\n",
       "      <td>References.pkl</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            PDF name  \\\n",
       "0                                 s12889-024-18580-0   \n",
       "1  Evaluation of the Lifetime Benefits of Metform...   \n",
       "2  regulatory_patterns_of_chinese_patent_medicine...   \n",
       "3  Association of Common Genetic Variants in Mito...   \n",
       "4                                 s00125-024-06144-1   \n",
       "5  Evidence that tirzepatide protects against dia...   \n",
       "6                                   fendo-14-1285147   \n",
       "7                                    fcdhc-03-947552   \n",
       "8  Visceral adiposity index as a predictor of typ...   \n",
       "\n",
       "                                             article      abstract  \\\n",
       "0  Prevalence of metabolic syndrome  and associat...  Abstract.pkl   \n",
       "1  Evaluation of the Lifetime Benefits of Metform...  Abstract.pkl   \n",
       "2  Regulatory patterns of Chinese patent medicine...  Abstract.pkl   \n",
       "3  Association of Common Genetic Variants in Mito...  Abstract.pkl   \n",
       "4  Subcutaneously administered tirzepatide vs sem...  Abstract.pkl   \n",
       "5  Evidence that tirzepatide protects against dia...  Abstract.pkl   \n",
       "6  Efficacy and safety of insulin glargine 300 un...  Abstract.pkl   \n",
       "7  Unpacking determinants and consequences of foo...  Abstract.pkl   \n",
       "8  Visceral adiposity index as a predictor of typ...  Abstract.pkl   \n",
       "\n",
       "             section 1                     section 2  \\\n",
       "0     Introduction.pkl                   Methods.pkl   \n",
       "1   1_Introduction.pkl                 2_Methods.pkl   \n",
       "2    1._Background.pkl                2._Methods.pkl   \n",
       "3     INTRODUCTION.pkl                   MetHops.pkl   \n",
       "4     Introduction.pkl                   Methods.pkl   \n",
       "5       Background.pkl                   Methods.pkl   \n",
       "6       Background.pkl                Objectives.pkl   \n",
       "7     Introduction.pkl                Objectives.pkl   \n",
       "8  1._Introduction.pkl  2._Materials_and_methods.pkl   \n",
       "\n",
       "                                           section 3  \\\n",
       "0                                         Result.pkl   \n",
       "1                                      3_Results.pkl   \n",
       "2                                     3._Results.pkl   \n",
       "3                                        Resutts.pkl   \n",
       "4                                        Results.pkl   \n",
       "5  Meta-analysis:_search_strategy,_selection_crit...   \n",
       "6                                        Methods.pkl   \n",
       "7  Diving_downstream:_Food_insecurity_as_a\\npoten...   \n",
       "8                                     3._Results.pkl   \n",
       "\n",
       "                                           section 4  \\\n",
       "0  Prevalence_of_metabolic_syndrome_among_type_2_...   \n",
       "1                                   4_Discussion.pkl   \n",
       "2                                  4._Conclusion.pkl   \n",
       "3                                     Discussion.pkl   \n",
       "4                                     Discussion.pkl   \n",
       "5                                        Results.pkl   \n",
       "6                                        Results.pkl   \n",
       "7              Implications_for_nutrition_equity.pkl   \n",
       "8                                  4._Discussion.pkl   \n",
       "\n",
       "                                           section 5  \\\n",
       "0  Association_between_BMI_and_prevalence_of_meta...   \n",
       "1                                   5_Conclusion.pkl   \n",
       "2                                  5._Conclusion.pkl   \n",
       "3                                     REFERENCES.pkl   \n",
       "4                                     References.pkl   \n",
       "5              Meta-analysis_of_clinical_results.pkl   \n",
       "6                          Results_of_the_search.pkl   \n",
       "7           Overview_of_study_setting_and_design.pkl   \n",
       "8                                  5._Conclusion.pkl   \n",
       "\n",
       "                                           section 6  \\\n",
       "0                                     Conclusion.pkl   \n",
       "1                                               None   \n",
       "2                           Author_contributions.pkl   \n",
       "3                                               None   \n",
       "4                                               None   \n",
       "5  Evidence_that_tirzepatide_protects\\nagainst_di...   \n",
       "6                                     Discussion.pkl   \n",
       "7       Blood_sampling,_processing,_and_analysis.pkl   \n",
       "8                         Authorsâ_contributions.pkl   \n",
       "\n",
       "                                           section 7  \\\n",
       "0                                               None   \n",
       "1                                               None   \n",
       "2                                     References.pkl   \n",
       "3                                               None   \n",
       "4                                               None   \n",
       "5  Cardioprotective_effect_of_TZT_by_meta-analysi...   \n",
       "6                                     Conclusion.pkl   \n",
       "7                                      Variables.pkl   \n",
       "8                                        Funding.pkl   \n",
       "\n",
       "                                           section 8  \\\n",
       "0                                               None   \n",
       "1                                               None   \n",
       "2                                               None   \n",
       "3                                               None   \n",
       "4                                               None   \n",
       "5  In_vitro_results_in_AC16_cell_line_results_in_...   \n",
       "6                    Data_availability_statement.pkl   \n",
       "7  Exit_interview_and_participant_follow-up_calls...   \n",
       "8  Disclosure_of_potential_conflicts_of_interest.pkl   \n",
       "\n",
       "                                           section 9  \\\n",
       "0                                               None   \n",
       "1                                               None   \n",
       "2                                               None   \n",
       "3                                               None   \n",
       "4                                               None   \n",
       "5  Protective_Effects_of_TZT_on_cell_proliferatio...   \n",
       "6                           Author_contributions.pkl   \n",
       "7                                       Variable.pkl   \n",
       "8                 Appendix_A._Supplementary_data.pkl   \n",
       "\n",
       "                        section 10                     section 11  \\\n",
       "0                             None                           None   \n",
       "1                             None                           None   \n",
       "2                             None                           None   \n",
       "3                             None                           None   \n",
       "4                             None                           None   \n",
       "5                  Conclusions.pkl  Supplementary_Information.pkl   \n",
       "6                   References.pkl                    Funding.pkl   \n",
       "7  Data_storage_and_management.pkl       Statistical_analysis.pkl   \n",
       "8                   References.pkl                           None   \n",
       "\n",
       "                section 12                section 13            section 14  \\\n",
       "0                     None                      None                  None   \n",
       "1                     None                      None                  None   \n",
       "2                     None                      None                  None   \n",
       "3                     None                      None                  None   \n",
       "4                     None                      None                  None   \n",
       "5     Publisherâs_Note.pkl                      None                  None   \n",
       "6      Acknowledgments.pkl  Conflict_of_interest.pkl  Publisher's_note.pkl   \n",
       "7  Insulin_sensitivity.pkl                      None                  None   \n",
       "8                     None                      None                  None   \n",
       "\n",
       "                   section 15  \n",
       "0                        None  \n",
       "1                        None  \n",
       "2                        None  \n",
       "3                        None  \n",
       "4                        None  \n",
       "5                        None  \n",
       "6  Supplementary_material.pkl  \n",
       "7                        None  \n",
       "8                        None  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "# Load the CSV file\n",
    "file_path = '/Diabetes PDFs Outputs/all_papers.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Define the keywords for section1\n",
    "section1_keywords = [\"introduction\", \"background\", \"aim\", \"objective\", \"introduction and background\"]\n",
    "\n",
    "def fuzzy_match(section, keywords, threshold=80):\n",
    "    for keyword in keywords:\n",
    "        if fuzz.partial_ratio(section.lower(), keyword.lower()) >= threshold:\n",
    "            return keyword.lower()\n",
    "    return None\n",
    "\n",
    "def reorder_sections(row):\n",
    "    # Extract fixed columns\n",
    "    fixed_columns = row[['PDF name', 'article', 'abstract']].tolist()\n",
    "    # Extract all other columns and drop NaN values\n",
    "    sections = row.drop(['PDF name', 'article', 'abstract']).dropna().tolist()\n",
    "    \n",
    "    # Create a list to hold the reordered sections\n",
    "    reordered_sections = []\n",
    "    abstract_idx = sections.index('Abstract.pkl') if 'Abstract.pkl' in sections else -1\n",
    "    \n",
    "    if abstract_idx != -1:\n",
    "        reordered_sections = sections[:abstract_idx+1]\n",
    "        remaining_sections = sections[abstract_idx+1:]\n",
    "    else:\n",
    "        remaining_sections = sections\n",
    "    \n",
    "    # Move specific sections to immediately follow the abstract section\n",
    "    sections_to_move = []\n",
    "    objective_sections = []\n",
    "    for sec in remaining_sections:\n",
    "        match = fuzzy_match(sec, section1_keywords)\n",
    "        if match in ['introduction', 'background']:\n",
    "            sections_to_move.append(sec)\n",
    "        elif match == 'objective':\n",
    "            objective_sections.append(sec)\n",
    "    \n",
    "    # Append moved sections in order\n",
    "    reordered_sections.extend(sections_to_move)\n",
    "    reordered_sections.extend(objective_sections)\n",
    "    \n",
    "    # Append the remaining sections after the moved sections\n",
    "    reordered_sections.extend([sec for sec in remaining_sections if sec not in sections_to_move + objective_sections])\n",
    "    \n",
    "    # Combine fixed columns and reordered sections\n",
    "    final_row = fixed_columns + reordered_sections\n",
    "    \n",
    "    # Ensure the final row has the same number of columns as the original\n",
    "    final_row_dict = {f'section {i+1}': final_row[i+3] if i+3 < len(final_row) else None for i in range(len(row) - 3)}\n",
    "    final_row_dict.update({'PDF name': fixed_columns[0], 'article': fixed_columns[1], 'abstract': fixed_columns[2]})\n",
    "    \n",
    "    return pd.Series(final_row_dict)\n",
    "\n",
    "# Apply the function to each row\n",
    "new_df = df.apply(reorder_sections, axis=1)\n",
    "\n",
    "# Ensure columns are ordered correctly\n",
    "new_df = new_df[['PDF name', 'article', 'abstract'] + [col for col in new_df.columns if col.startswith('section')]]\n",
    "\n",
    "# Save the new dataframe to a CSV file\n",
    "output_path = '/Diabetes PDFs Outputs/reordered_all_papers.csv'\n",
    "new_df.to_csv(output_path, index=False)\n",
    "\n",
    "# new_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process\n",
    "import requests\n",
    "\n",
    "# è¯»åall_papers.csv\n",
    "all_papers_path = '/Diabetes PDFs Outputs/reordered_all_papers.csv'\n",
    "all_papers_df = pd.read_csv(all_papers_path)\n",
    "\n",
    "section1_keywords = [\"introduction\", \"background\", \"aim\", \"objective\", \"introduction and background\"]\n",
    "section2_keywords = [\"results\", \"conclusion\", \"discussion\", \"limitations\"]\n",
    "section3_keywords = [\"methods\", \"study design\"]\n",
    "remove_section_keywords = [\"acknowledgement\", \"conflict of interest\", \"funding\", \"references\", \"appendix\", \"contributors\", \"author\", \"data source\", \"data availability\"]\n",
    "\n",
    "def load_text(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        return file.read()\n",
    "\n",
    "def save_combined_content(content, filename, folder):\n",
    "    txt_path = os.path.join(folder, filename + '.txt')\n",
    "    pkl_path = os.path.join(folder, filename + '.pkl')\n",
    "    with open(txt_path, 'w', encoding='utf-8') as txt_file:\n",
    "        txt_file.write(content)\n",
    "    with open(pkl_path, 'wb') as pkl_file:\n",
    "        pickle.dump(content, pkl_file)\n",
    "\n",
    "def match_keyword(text, keywords):\n",
    "    best_match, score = process.extractOne(text, keywords, scorer=fuzz.partial_ratio)\n",
    "    return best_match if score >= 80 else None\n",
    "\n",
    "def classify_sections(df):\n",
    "    new_rows = []\n",
    "    for _, row in df.iterrows():\n",
    "        pdf_name = row['PDF name']\n",
    "        article = row['article']\n",
    "        abstract = row['abstract']\n",
    "        \n",
    "        sections = [col for col in row.index if col.startswith('section') and pd.notna(row[col])]\n",
    "        \n",
    "        section1 = []\n",
    "        section2 = []\n",
    "        section3 = []\n",
    "        \n",
    "        # Step 1: Remove all sections with remove_keywords\n",
    "        sections = [sec for sec in sections if not match_keyword(row[sec].lower(), remove_section_keywords)]\n",
    "        \n",
    "        # Step 2: Collect section1\n",
    "        section1_collecting = True\n",
    "        for section in sections:\n",
    "            section_title = row[section].lower()\n",
    "            if match_keyword(section_title, section1_keywords):\n",
    "                section1.append(section)\n",
    "            elif match_keyword(section_title, section2_keywords):\n",
    "                section1_collecting = False\n",
    "                break\n",
    "        \n",
    "        # Step 3: Collect section2\n",
    "        section2_collecting = False\n",
    "        section2_start = None\n",
    "        section2_end = None\n",
    "        for section in sections:\n",
    "            section_title = row[section].lower()\n",
    "            if match_keyword(section_title, section2_keywords):\n",
    "                if section2_start is None:\n",
    "                    section2_start = section\n",
    "                section2_end = section\n",
    "                section2_collecting = True\n",
    "        \n",
    "        if section2_start and section2_end:\n",
    "            section2_start_index = sections.index(section2_start)\n",
    "            section2_end_index = sections.index(section2_end)\n",
    "            section2 = sections[section2_start_index:section2_end_index + 1]\n",
    "        \n",
    "        # Step 4: Collect section3 (remaining sections between section1 and section2)\n",
    "        if section1 and section2:\n",
    "            section1_end_index = sections.index(section1[-1])\n",
    "            section2_start_index = sections.index(section2[0])\n",
    "            section3 = sections[section1_end_index + 1 : section2_start_index]\n",
    "            section3 = [s for s in section3 if not match_keyword(row[s].lower(), remove_section_keywords)]\n",
    "        else:\n",
    "            section3 = []\n",
    "\n",
    "        # Ensure section3 has content if empty\n",
    "        if not section3:\n",
    "            section3 = [sec for sec in sections if match_keyword(row[sec].lower(), section3_keywords)]\n",
    "        \n",
    "        new_row = {\n",
    "            'PDF name': pdf_name,\n",
    "            'article': article,\n",
    "            'abstract': abstract,\n",
    "            'section1': ','.join(section1),\n",
    "            'section2': ','.join(section2),\n",
    "            'section3': ','.join(section3)\n",
    "        }\n",
    "        new_rows.append(new_row)\n",
    "    \n",
    "    return pd.DataFrame(new_rows)\n",
    "\n",
    "def remove_useless_info(text):\n",
    "    lines = text.split('\\n')\n",
    "    useful_lines = []\n",
    "    for line in lines:\n",
    "        if any(keyword in line.lower() for keyword in remove_section_keywords):\n",
    "            break\n",
    "        useful_lines.append(line)\n",
    "    return '\\n'.join(useful_lines)\n",
    "\n",
    "\n",
    "classified_df = classify_sections(all_papers_df)\n",
    "\n",
    "rows = []\n",
    "\n",
    "# Process each pdf folder\n",
    "for _, row in classified_df.iterrows():\n",
    "    pdf_name = row['PDF name']\n",
    "    article = row['article']\n",
    "    abstract_filename = row['abstract']\n",
    "    \n",
    "    section1_sections = row['section1'].split(',')\n",
    "    section2_sections = row['section2'].split(',')\n",
    "    section3_sections = row['section3'].split(',')\n",
    "    \n",
    "    pdf_folder = os.path.join('Diabetes PDFs Outputs', pdf_name)\n",
    "    text_folder = os.path.join(pdf_folder, 'text')\n",
    "    pickle_folder = os.path.join(pdf_folder, 'pickle')\n",
    "    combined_folder = os.path.join(pdf_folder, 'combined')\n",
    "    os.makedirs(combined_folder, exist_ok=True)\n",
    "    \n",
    "    abstract_text = load_text(os.path.join(text_folder, abstract_filename.replace('.pkl', '.txt')))\n",
    "    \n",
    "    section1_text = \"\"\n",
    "    section2_text = \"\"\n",
    "    section3_text = \"\"\n",
    "    \n",
    "    for section in section1_sections:\n",
    "        if section:\n",
    "            section1_text += load_text(os.path.join(text_folder, all_papers_df.loc[all_papers_df['PDF name'] == pdf_name, section].values[0].replace('.pkl', '.txt')))\n",
    "    \n",
    "    for section in section2_sections:\n",
    "        if section:\n",
    "            section2_text += load_text(os.path.join(text_folder, all_papers_df.loc[all_papers_df['PDF name'] == pdf_name, section].values[0].replace('.pkl', '.txt')))\n",
    "    \n",
    "    for section in section3_sections:\n",
    "        if section:\n",
    "            section3_text += load_text(os.path.join(text_folder, all_papers_df.loc[all_papers_df['PDF name'] == pdf_name, section].values[0].replace('.pkl', '.txt')))\n",
    "    \n",
    "\n",
    "    # print(f\"PDF: {pdf_name}\")\n",
    "    # print(f\"Section 1 Content: {section1_text[:500]}\")  \n",
    "    # print(f\"Section 2 Content: {section2_text[:500]}\")  \n",
    "    # print(f\"Section 3 Content: {section3_text[:500]}\") \n",
    "    # print(\"\\n\")\n",
    "    \n",
    "    # Save\n",
    "    save_combined_content(abstract_text, 'Abstract', combined_folder)\n",
    "    save_combined_content(section1_text, 'Section1', combined_folder)\n",
    "    save_combined_content(section2_text, 'Section2', combined_folder)\n",
    "    save_combined_content(section3_text, 'Section3', combined_folder)\n",
    "\n",
    "    # Get citation\n",
    "    citation = get_citation(article)\n",
    "    \n",
    "    # Collect row data \n",
    "    rows.append({\n",
    "        'PDF name': pdf_name,\n",
    "        'article': article,\n",
    "        'abstract': os.path.join(combined_folder, 'Abstract.pkl'),\n",
    "        'section1': os.path.join(combined_folder, 'Section1.pkl'),\n",
    "        'section2': os.path.join(combined_folder, 'Section2.pkl'),\n",
    "        'section3': os.path.join(combined_folder, 'Section3.pkl'),\n",
    "        'citation': citation\n",
    "    })\n",
    "\n",
    "new_df = pd.DataFrame(rows)\n",
    "new_df.to_csv(os.path.join('Diabetes PDFs Outputs', 'combined_papers_info.csv'), index=False)\n",
    "# print(\"successfully\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF: s12889-024-18580-0\n",
      "Section 1 Content: Introduction The complicated pathophysiologic condition known as the metabolic syndrome is characterised by insulin resis- tance, hypertension, hyperlipidaemia, and abdominal obesity and which originate primarily from an imbalance between energy expenditure and calorie intake [1]. Even though the NCEP-ATPIII, IDF, and WHO criteria are the most often utilised clinical criteria for the diagnosis of metabolic syndrome, there are numerous similarities between them, there are also notable differences\n",
      "Section 2 Content: \n",
      "Section 3 Content: \n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/billionaire/opt/anaconda3/envs/detectron2/lib/python3.9/site-packages/urllib3/connectionpool.py:1103: InsecureRequestWarning: Unverified HTTPS request is being made to host 'api.crossref.org'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF: Evaluation of the Lifetime Benefits of Metformin and SGLT2 Inhibitors in Type 2 Diabetes Mellitus Patients with Cardiovascular Disease A Systematic Review and Two-Stage Meta-Analysis\n",
      "Section 1 Content: 1 Introduction The incidence of type 2 diabetes mellitus (T2DM) is on the rise globally. There are an estimated 537 million diagnosed cases, and by 2045, this number will rise to 783 million [1]. Patients with T2DM and established cardiovascular disease (CVD) are particularly susceptible to recurrent major adverse cardiovascular events (MACE), with a 1.7-fold increase in risk [2]. These patients are also at a higher risk for cardiovascular and non-cardiovascular complications. Optimizing glycaem\n",
      "Section 2 Content: \n",
      "Section 3 Content: \n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/billionaire/opt/anaconda3/envs/detectron2/lib/python3.9/site-packages/urllib3/connectionpool.py:1103: InsecureRequestWarning: Unverified HTTPS request is being made to host 'api.crossref.org'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF: regulatory_patterns_of_chinese_patent_medicine_for.26\n",
      "Section 1 Content: 1. Background The World Health Organization defines âcomorbidityâ as the coexistence of 2 or more health conditions that necessitate ongoing and diverse treatments and can mutually influence each other, In China, the prevalence of chronic diseases is 69.13%, and the comorbidity rate is 43.65%. Among these, multiple dis- eases commonly coexist among elderly patients with type 2 dia- betes.!\") Recent data from the British Medical Journal indicates that the prevalence of comorbidity in high-income \n",
      "Section 2 Content: 3. Results 3.1. Study identification 3.1. Study identification In this study, a total of 80 English articles and 792 Chinese articles were initially identified through the screening process. Articles that did not meet the criteria were excluded based on a review of their titles and abstracts. Additionally, CPMs with <2 relevant articles were excluded. Ultimately, 12 RCTs!*! were included for analysis. A detailed description of the screening, process can be found in Figure 1. 3.2. Characteristics\n",
      "Section 3 Content: \n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/billionaire/opt/anaconda3/envs/detectron2/lib/python3.9/site-packages/urllib3/connectionpool.py:1103: InsecureRequestWarning: Unverified HTTPS request is being made to host 'api.crossref.org'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF: Association of Common Genetic Variants in Mitogenâactivated Protein Kinase Kinase Kinase Kinase 4 with Type 2 Diabetes Mellitus in a Chinese Han Population\n",
      "Section 1 Content: \n",
      "Section 2 Content: Resutts Clinical and biochemical characteristics of study subjects There were four subjects in T2DM group and 24 subjects in controls have not detected the genotypes, so finally the T2DM group includes 996 subjects, and the control group includes 976 subjects. The clinical characteristics of the participants are shown in Table 1. There were 612 males and 383 females (mean age, 46.1 + 12.6 years) in the patients and 568 males and 399 females (mean age, 42.9 + 11.7 years) in the control subjects. \n",
      "Section 3 Content: MetHops Study subjects and phenotypic definitions Using a case-control approach, a total of 2000 unrelated subjects from Chinese Han population were recruited from October 2010 to September 2013, comprising 1000 T2DM patients and 1000 normoglycemic control subjects. Inclusion criteria for this study were as follows: (1) All subject ages were between 20 and 79 years; (2) The diagnoses of T2DM patients were according to the World Health Organization 1999 with specified standards; (3) For eliminati\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/billionaire/opt/anaconda3/envs/detectron2/lib/python3.9/site-packages/urllib3/connectionpool.py:1103: InsecureRequestWarning: Unverified HTTPS request is being made to host 'api.crossref.org'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF: s00125-024-06144-1\n",
      "Section 1 Content: \n",
      "Section 2 Content: \n",
      "Section 3 Content: Methods The protocol of this systematic review and meta- analysis is registered in PROSPERO (registration no. CRD42022382594) [9]. We report our methods and result: in accordance with the Preferred Reporting Items for Sys tematic reviews and Meta-Analyses (PRISMA) statement for network meta-analyses [10]. ty criteria We included RCTs published in English sed s.c. tirzepatide at maintenance doses of 5 mg, 10 mg or 15 mg once weekly, or s.c. semaglutide at main- tenance doses of 0.5 mg, 1.0 mg or \n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/billionaire/opt/anaconda3/envs/detectron2/lib/python3.9/site-packages/urllib3/connectionpool.py:1103: InsecureRequestWarning: Unverified HTTPS request is being made to host 'api.crossref.org'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF: Evidence that tirzepatide protects against diabetes-related cardiac damages\n",
      "Section 1 Content: Background Glucagon-like peptide-1 receptor agonists (GLP- 1RAs), widely used antidiabetic drugs, are approved and recommended in several treatment guidelines for reducing the risk of major adverse cardiovascular events (MACE), such as cardiovascular death, non-fatal myocardial infarction (MI), and non-fatal stroke [1-6]. However, unlike sodium-glucose co-transporter 2 (SGLT2) inhibitors [7], evidence of a benefit for GLP-1 RAs in heart failure (HF) is controversial and remains to be fully estab\n",
      "Section 2 Content: ResultsMeta-analysis of clinical resultsEvidence that tirzepatide protects against diabetes-related cardiac damages Fatemeh Taktaz'*, Lucia Scisciola'ââ, Rosaria Anna Fontanella', Ada Pesapane', Puja Ghoshâ, Martina Franzese', Giovanni Tortorella', Armando Puocci!, Eduardo Sommella?, Giuseppe Signoriello*, Fabiola Olivieri*Â®, Michelangela Barbieri'* and Giuseppe Paolisso'**Cardioprotective effect of TZT by meta-analysis re â Data from 7778 patients enrolled in the SURPASS-4 study (SURPASS-4 [17]\n",
      "Section 3 Content: MethodsMeta-analysis: search strategy, selection criteria, endpoint, and statistical analysis A systematic literature review was conducted by searching the PubMed database for randomized clinical trials from 2018 to December 2023. The review included 7778 adult patients, regardless of their diabetes mellitus status at baseline, who were assigned to either TZT or placebo/active control. Data from published reports and previous meta-analyses were utilized, and all included manuscripts were manuall\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/billionaire/opt/anaconda3/envs/detectron2/lib/python3.9/site-packages/urllib3/connectionpool.py:1103: InsecureRequestWarning: Unverified HTTPS request is being made to host 'api.crossref.org'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF: fendo-14-1285147\n",
      "Section 1 Content: Background Description of the condition More than 500 million people are living with diabetes. Furthermore, the number of people with diabetes is expected to reach 643 million by 2030 and 783 million by 2045. In 2021, 6.7 million deaths were attributed to diabetes (1). Diabetes is currently the greatest pandemic of the 21st century according to many epidemiologists (2, 3). The high global prevalence of diabetes has a vexing impact on individuals, healthcare systems, and countries all over the gl\n",
      "Section 2 Content: \n",
      "Section 3 Content: \n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/billionaire/opt/anaconda3/envs/detectron2/lib/python3.9/site-packages/urllib3/connectionpool.py:1103: InsecureRequestWarning: Unverified HTTPS request is being made to host 'api.crossref.org'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF: fcdhc-03-947552\n",
      "Section 1 Content: Introduction The fourth decade of the HIV epidemic marks many milestones in the treatment and management of HIV as a chronic disease. These medical advances have paved the way for new questions about how to optimize the health span for people living with HIV (PLWH), including chronic co- morbidity risk reduction and management. PLWH experience higher rates of diabetes compared to the general US population (1), with one in ten PLWH having diabetes (1) and another three in ten PLWH having prediabe\n",
      "Section 2 Content: \n",
      "Section 3 Content: \n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/billionaire/opt/anaconda3/envs/detectron2/lib/python3.9/site-packages/urllib3/connectionpool.py:1103: InsecureRequestWarning: Unverified HTTPS request is being made to host 'api.crossref.org'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF: Visceral adiposity index as a predictor of type 2 diabetes mellitus risk- A systematic review and doseeresponse meta-analysis\n",
      "Section 1 Content: 1. Introduction Diabetes mellitus remains a serious and growing challenge to public health and places a huge burden on individuals affected and their families. According to the International Diabetes Federation, in 2021, it is estimated that 537 million (10.5% of the population) people have diabetes, and this number is projected to reach 643 million (11.3%) by 2030, and 783 million (12.2%) by 2045 [1]. While Type 2 diabetes mellitus (T2DM) accounts for the vast majority (over 90%) of diabetes wo\n",
      "Section 2 Content: 3. Results 3.1. Study selection and characteristics of included studies 3.1. Study selection and characteristics of included studies A total of 508 potentially relevant records were identified in our literature. According to the pre-specified inclusion criteria, 14 studies [14,29â41] included in our analysis (Fig. 1). Overall, A total of 9 prospective cohort studies and 5 cross sectional studies were included in our review, comprising a total 13,339 participants and 1655 T2DM events in cross-sec\n",
      "Section 3 Content: \n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/billionaire/opt/anaconda3/envs/detectron2/lib/python3.9/site-packages/urllib3/connectionpool.py:1103: InsecureRequestWarning: Unverified HTTPS request is being made to host 'api.crossref.org'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF: Prognostic Significance of Diastolic Dysfunction in Type 2 Diabetes Mellitus Patients With Sepsis and Septic Shock- Insights From a Longitudinal Tertiary Care Study\n",
      "Section 1 Content: Introduction Sepsis affects 48.9 million people per year globally, of which 11 million die, representing a mortality rate of 19.7% [1]. According to the Third International Consensus Definitions for Sepsis and Septic Shock (Sepsis-  is defined as a life-threatening dysfunction caused by the dysregulated host response to  sociated with a >10% in-hospital mortality. \"Septic shock is defined as the subset of sepsis in which particularly profound circulatory, cellular, and metabolic abnormalities ar\n",
      "Section 2 Content: Results A total of 132 patients were enrolled after the exclusion of 12 patients with poor echocardiography images, 10 patients with known cases of coronary artery disease showing regional wall abnormalities, and five patients with moderate to severe valvular heart disease. The main source of sepsis was community-acquire pneumonia (41.60%), complicated urinary tract infections (21.20%), hospital-acquired pneumonia (12.80% gastrointestinal infections (6%), skin and soft tissue infections (4%), an\n",
      "Section 3 Content: Materials And Methods This observational cohort study was conducted at a tertiary care center in northern India over 18 months from July 2022 to November 2022 after obtaining approval from the Institutional Ethics Committee, All Indi Institute of Medical Sciences, Rishikesh (AIIMS/IEC/2/321). T2DM was defined based on the American Diabetes Association (ADA) criteria, which include an elevated glycated hemoglobin Alc (HbAlc) level of 6.5% or higher, a random blood sugar (RBS) measurement equal to\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/billionaire/opt/anaconda3/envs/detectron2/lib/python3.9/site-packages/urllib3/connectionpool.py:1103: InsecureRequestWarning: Unverified HTTPS request is being made to host 'api.crossref.org'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ´ååä¿å­å®æã\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process\n",
    "import requests\n",
    "\n",
    "# è¯»åall_papers.csv\n",
    "all_papers_path = '/Users/billionaire/Desktop/LLM/OCR/Diabetes PDFs Outputs/reordered_all_papers.csv'\n",
    "all_papers_df = pd.read_csv(all_papers_path)\n",
    "\n",
    "section1_keywords = [\"introduction\", \"background\", \"aim\", \"objective\"]\n",
    "section2_keywords = [\"results\", \"conclusion\", \"discussion\", \"limitations\"]\n",
    "section3_keywords = [\"methods\", \"study design\"]\n",
    "remove_section_keywords = [\"acknowledgement\", \"conflict of interest\", \"funding\", \"references\", \"appendix\", \"contributors\", \"author\", \"data source\", \"data availability\"]\n",
    "\n",
    "def load_text(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        return file.read()\n",
    "\n",
    "def save_combined_content(content, filename, folder):\n",
    "    txt_path = os.path.join(folder, filename + '.txt')\n",
    "    pkl_path = os.path.join(folder, filename + '.pkl')\n",
    "    with open(txt_path, 'w', encoding='utf-8') as txt_file:\n",
    "        txt_file.write(content)\n",
    "    with open(pkl_path, 'wb') as pkl_file:\n",
    "        pickle.dump(content, pkl_file)\n",
    "\n",
    "def match_keyword(text, keywords):\n",
    "    best_match, score = process.extractOne(text, keywords, scorer=fuzz.partial_ratio)\n",
    "    return best_match if score >= 80 else None\n",
    "\n",
    "def classify_sections(df):\n",
    "    new_rows = []\n",
    "    for _, row in df.iterrows():\n",
    "        pdf_name = row['PDF name']\n",
    "        article = row['article']\n",
    "        abstract = row['abstract']\n",
    "        \n",
    "        sections = [col for col in row.index if col.startswith('section') and pd.notna(row[col])]\n",
    "        \n",
    "        section1 = []\n",
    "        section2 = []\n",
    "        section3 = []\n",
    "        \n",
    "        # Step 1: Remove all sections with remove_keywords\n",
    "        sections = [sec for sec in sections if not match_keyword(row[sec].lower(), remove_section_keywords)]\n",
    "        \n",
    "        # Step 2: Collect section1\n",
    "        section1_collecting = True\n",
    "        for section in sections:\n",
    "            section_title = row[section].lower()\n",
    "            if match_keyword(section_title, section1_keywords):\n",
    "                section1.append(section)\n",
    "            elif match_keyword(section_title, section2_keywords):\n",
    "                section1_collecting = False\n",
    "                break\n",
    "        \n",
    "        # Step 3: Collect section2\n",
    "        section2_collecting = False\n",
    "        section2_start = None\n",
    "        section2_end = None\n",
    "        for section in sections:\n",
    "            section_title = row[section].lower()\n",
    "            if match_keyword(section_title, section2_keywords):\n",
    "                if section2_start is None:\n",
    "                    section2_start = section\n",
    "                section2_end = section\n",
    "                section2_collecting = True\n",
    "        \n",
    "        if section2_start and section2_end:\n",
    "            section2_start_index = sections.index(section2_start)\n",
    "            section2_end_index = sections.index(section2_end)\n",
    "            section2 = sections[section2_start_index:section2_end_index + 1]\n",
    "        \n",
    "        # Step 4: Collect section3 (remaining sections between section1 and section2)\n",
    "        if section1 and section2:\n",
    "            section1_end_index = sections.index(section1[-1])\n",
    "            section2_start_index = sections.index(section2[0])\n",
    "            section3 = sections[section1_end_index + 1 : section2_start_index]\n",
    "            section3 = [s for s in section3 if not match_keyword(row[s].lower(), remove_section_keywords)]\n",
    "        else:\n",
    "            section3 = []\n",
    "\n",
    "        # Ensure section3 has content if empty\n",
    "        if not section3:\n",
    "            section3 = [sec for sec in sections if match_keyword(row[sec].lower(), section3_keywords)]\n",
    "        \n",
    "        new_row = {\n",
    "            'PDF name': pdf_name,\n",
    "            'article': article,\n",
    "            'abstract': abstract,\n",
    "            'section1': ','.join(section1),\n",
    "            'section2': ','.join(section2),\n",
    "            'section3': ','.join(section3)\n",
    "        }\n",
    "        new_rows.append(new_row)\n",
    "    \n",
    "    return pd.DataFrame(new_rows)\n",
    "\n",
    "def remove_useless_info(text):\n",
    "    lines = text.split('\\n')\n",
    "    useful_lines = []\n",
    "    for line in lines:\n",
    "        if any(keyword in line.lower() for keyword in remove_section_keywords):\n",
    "            break\n",
    "        useful_lines.append(line)\n",
    "    return '\\n'.join(useful_lines)\n",
    "\n",
    "classified_df = classify_sections(all_papers_df)\n",
    "\n",
    "rows = []\n",
    "\n",
    "# Process each pdf folder\n",
    "for _, row in classified_df.iterrows():\n",
    "    pdf_name = row['PDF name']\n",
    "    article = row['article']\n",
    "    abstract_filename = row['abstract']\n",
    "    \n",
    "    section1_sections = row['section1'].split(',')\n",
    "    section2_sections = row['section2'].split(',')\n",
    "    section3_sections = row['section3'].split(',')\n",
    "    \n",
    "    pdf_folder = os.path.join('Diabetes PDFs Outputs', pdf_name)\n",
    "    text_folder = os.path.join(pdf_folder, 'text')\n",
    "    pickle_folder = os.path.join(pdf_folder, 'pickle')\n",
    "    combined_folder = os.path.join(pdf_folder, 'combined')\n",
    "    os.makedirs(combined_folder, exist_ok=True)\n",
    "    \n",
    "\n",
    "    abstract_text = load_text(os.path.join(text_folder, abstract_filename.replace('.pkl', '.txt')))\n",
    "    \n",
    "    section1_text = \"\"\n",
    "    section2_text = \"\"\n",
    "    section3_text = \"\"\n",
    "    \n",
    "    for section in section1_sections:\n",
    "        if section:\n",
    "            section1_text += load_text(os.path.join(text_folder, all_papers_df.loc[all_papers_df['PDF name'] == pdf_name, section].values[0].replace('.pkl', '.txt')))\n",
    "    \n",
    "    for section in section2_sections:\n",
    "        if section:\n",
    "            section2_text += load_text(os.path.join(text_folder, all_papers_df.loc[all_papers_df['PDF name'] == pdf_name, section].values[0].replace('.pkl', '.txt')))\n",
    "    \n",
    "    for section in section3_sections:\n",
    "        if section:\n",
    "            section3_text += load_text(os.path.join(text_folder, all_papers_df.loc[all_papers_df['PDF name'] == pdf_name, section].values[0].replace('.pkl', '.txt')))\n",
    "    \n",
    "    # Remove useless information\n",
    "    section1_text = remove_useless_info(section1_text)\n",
    "    section2_text = remove_useless_info(section2_text)\n",
    "    section3_text = remove_useless_info(section3_text)\n",
    "\n",
    "    # print(f\"PDF: {pdf_name}\")\n",
    "    # print(f\"Section 1 Content: {section1_text[:500]}\")  \n",
    "    # print(f\"Section 2 Content: {section2_text[:500]}\")  \n",
    "    # print(f\"Section 3 Content: {section3_text[:500]}\")  \n",
    "    # print(\"\\n\")\n",
    "    \n",
    "    save_combined_content(abstract_text, 'Abstract', combined_folder)\n",
    "    save_combined_content(section1_text, 'Section1', combined_folder)\n",
    "    save_combined_content(section2_text, 'Section2', combined_folder)\n",
    "    save_combined_content(section3_text, 'Section3', combined_folder)\n",
    "\n",
    "    # get citation\n",
    "    citation = get_citation(article)\n",
    "    \n",
    "    rows.append({\n",
    "        'PDF name': pdf_name,\n",
    "        'article': article,\n",
    "        'abstract': os.path.join(combined_folder, 'Abstract.pkl'),\n",
    "        'section1': os.path.join(combined_folder, 'Section1.pkl'),\n",
    "        'section2': os.path.join(combined_folder, 'Section2.pkl'),\n",
    "        'section3': os.path.join(combined_folder, 'Section3.pkl'),\n",
    "        'citation': citation\n",
    "    })\n",
    "\n",
    "new_df = pd.DataFrame(rows)\n",
    "\n",
    "new_df.to_csv(os.path.join('Diabetes PDFs Outputs', 'combined_papers_info.csv'), index=False)\n",
    "\n",
    "print(\"successfullyã\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "detectron2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
